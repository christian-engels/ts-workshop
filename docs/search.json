[
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html",
    "href": "day2_part3_application_to_stock_portfolio.html",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "",
    "text": "In this session, we will apply time series econometric techniques to an S&P 500 stock portfolio. The objectives are:\n\nDownload and preprocess S&P 500 stock data.\nConstruct a minimum variance portfolio.\nFit a GARCH model to the portfolio returns.\nEvaluate the model and perform rolling forecasts.\nAssess Value at Risk (VaR) using the fitted model."
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#introduction",
    "href": "day2_part3_application_to_stock_portfolio.html#introduction",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "",
    "text": "In this session, we will apply time series econometric techniques to an S&P 500 stock portfolio. The objectives are:\n\nDownload and preprocess S&P 500 stock data.\nConstruct a minimum variance portfolio.\nFit a GARCH model to the portfolio returns.\nEvaluate the model and perform rolling forecasts.\nAssess Value at Risk (VaR) using the fitted model."
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#loading-required-libraries",
    "href": "day2_part3_application_to_stock_portfolio.html#loading-required-libraries",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\nWe begin by loading the necessary R packages for data manipulation, time series analysis, portfolio optimization, and modeling.\n\nlibrary(tidyverse)         # Data manipulation and visualization\nlibrary(tidyfinance)       # Financial data tools\nlibrary(tsibble)           # Time series data manipulation\nlibrary(feasts)            # Time series features\nlibrary(rugarch)           # Univariate GARCH models\nlibrary(PortfolioAnalytics) # Portfolio optimization\nlibrary(xts)               # Extensible time series"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#data-acquisition",
    "href": "day2_part3_application_to_stock_portfolio.html#data-acquisition",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Data Acquisition",
    "text": "Data Acquisition\n\nDownloading S&P 500 Constituents and Prices\nWe download the S&P 500 constituents and their historical adjusted closing prices. If the data already exists locally, we read it from the CSV file to save time.\n\nif (!file.exists(\"sp500_download.csv\")) {\n  \n  # Download S&P 500 constituents\n  constituents &lt;- download_data(\"constituents\", index = \"S&P 500\")\n  \n  # Download historical stock prices for all constituents\n  sp500_download &lt;-\n    download_data(\n      \"stock_prices\",\n      symbols = constituents %&gt;% pull(symbol),\n      start_date = \"2014-11-18\",\n      end_date = \"2024-11-18\"\n    )\n  \n  # Save the data to CSV for future use\n  sp500_download %&gt;% write_csv(\"sp500_download.csv\")\n} else {\n  # Read the data from CSV if it exists\n  sp500_download &lt;- read_csv(\"sp500_download.csv\")\n}\n\nRows: 1223542 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): symbol\ndbl  (6): volume, open, low, high, close, adjusted_close\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#data-preparation",
    "href": "day2_part3_application_to_stock_portfolio.html#data-preparation",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nCalculating Daily Returns\nWe calculate the daily returns for each stock in the S&P 500 index. We also ensure that we only keep stocks with complete data over the entire period.\n\nsp500_returns &lt;-\n  sp500_download %&gt;%\n  select(symbol, date, adjusted_close) %&gt;%\n  group_by(symbol) %&gt;%\n  arrange(date) %&gt;% # Ensure data is sorted by date\n  mutate(\n    return = adjusted_close / lag(adjusted_close) - 1\n  ) %&gt;%\n  ungroup() %&gt;%\n  drop_na(return) %&gt;% # Remove NAs resulting from lag\n  group_by(symbol) %&gt;%\n  mutate(n_days = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(n_days == max(n_days)) %&gt;% # Keep symbols with full data\n  select(symbol, date, return)\n\n\n\nTransforming Data to Wide Format\nWe pivot the data to a wide format where each column represents the returns of a stock, and each row represents a date.\n\nsp500_wide &lt;-\n  sp500_returns %&gt;%\n  pivot_wider(\n    id_cols = date,\n    values_from = return,\n    names_from = symbol\n  ) %&gt;%\n  mutate(date = as_date(date))\nsp500_wide\n\n# A tibble: 2,515 × 469\n   date            NVDA      AAPL     MSFT      AMZN     META    GOOGL      TSLA\n   &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 2014-11-19 -0.00373  -0.00693  -0.0107   0.00495  -0.0136   0.00494 -0.0386  \n 2 2014-11-20  0.0165    0.0143    0.00995  0.0122    0.00368 -0.00629  0.00392 \n 3 2014-11-21  0.00541   0.00138  -0.0148   0.00632   0.00204  0.00392 -0.0238  \n 4 2014-11-24  0.00636   0.0185   -0.00813  0.00905   0.00353  0.00291  0.0162  \n 5 2014-11-25 -0.000486 -0.00868  -0.00252 -0.00179   0.0219   0.00320  0.00555 \n 6 2014-11-26  0.0170    0.0119    0.00590 -0.00439   0.0263  -0.00273  0.00141 \n 7 2014-11-28  0.00239  -0.000588  0.00126  0.0152    0.00103  0.00246 -0.0158  \n 8 2014-12-01 -0.0186   -0.0325    0.0169  -0.0373   -0.0335  -0.0172  -0.0527  \n 9 2014-12-02  0.00146  -0.00382  -0.00329  0.000951  0.00479 -0.00196 -0.000907\n10 2014-12-03  0.0257    0.0113   -0.00784 -0.0301   -0.00769 -0.00301 -0.00920 \n# ℹ 2,505 more rows\n# ℹ 461 more variables: GOOG &lt;dbl&gt;, AVGO &lt;dbl&gt;, JPM &lt;dbl&gt;, LLY &lt;dbl&gt;,\n#   UNH &lt;dbl&gt;, XOM &lt;dbl&gt;, V &lt;dbl&gt;, MA &lt;dbl&gt;, COST &lt;dbl&gt;, HD &lt;dbl&gt;, PG &lt;dbl&gt;,\n#   NFLX &lt;dbl&gt;, WMT &lt;dbl&gt;, JNJ &lt;dbl&gt;, CRM &lt;dbl&gt;, BAC &lt;dbl&gt;, ORCL &lt;dbl&gt;,\n#   ABBV &lt;dbl&gt;, CVX &lt;dbl&gt;, WFC &lt;dbl&gt;, MRK &lt;dbl&gt;, KO &lt;dbl&gt;, CSCO &lt;dbl&gt;,\n#   ACN &lt;dbl&gt;, ADBE &lt;dbl&gt;, AMD &lt;dbl&gt;, PEP &lt;dbl&gt;, LIN &lt;dbl&gt;, NOW &lt;dbl&gt;,\n#   DIS &lt;dbl&gt;, MCD &lt;dbl&gt;, IBM &lt;dbl&gt;, ABT &lt;dbl&gt;, PM &lt;dbl&gt;, TMO &lt;dbl&gt;, …\n\n\n\n\nPreparing Data for Portfolio Optimization\nWe convert the returns data into a data frame suitable for portfolio optimization. The returns data frame will have dates as row names and stock symbols as column names.\n\nreturns &lt;- sp500_wide %&gt;%\n  select(-date) %&gt;%\n  as.data.frame()\nrownames(returns) &lt;- sp500_wide$date\nfunds &lt;- colnames(returns)"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#portfolio-optimization",
    "href": "day2_part3_application_to_stock_portfolio.html#portfolio-optimization",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Portfolio Optimization",
    "text": "Portfolio Optimization\n\nSpecifying Portfolio Constraints and Objectives\nWe define the portfolio specification, including the constraints and the objective to minimize portfolio variance (standard deviation).\n\ninit_portf &lt;-\n  portfolio.spec(assets = funds) %&gt;%\n  add.constraint(type = \"weight_sum\", min_sum = 0.99, max_sum = 1.01) %&gt;%\n  add.constraint(type = \"box\", min = 0, max = 1) %&gt;%\n  add.objective(type = \"risk\", name = \"StdDev\")\n\n\nWeight Sum Constraint: The sum of the portfolio weights should be approximately 1.\nBox Constraint: Each asset weight should be between 0 and 1 (no short selling).\nObjective: Minimize the portfolio standard deviation (risk).\n\n\n\nOptimizing the Portfolio\nWe use the optimize.portfolio function to find the portfolio weights that minimize the portfolio’s standard deviation.\n\nmin_sd &lt;-\n  optimize.portfolio(\n    R = returns,\n    portfolio = init_portf,\n    optimize_method = \"osqp\",\n    trace = TRUE\n  )\n\nNote: The osqp method is used for optimization due to its efficiency in handling quadratic programming problems.\n\n\nExtracting Optimal Weights\nWe extract and round the optimal portfolio weights for easier interpretation.\n\nweights &lt;-\n  tibble(\n    symbol = names(min_sd$weights),\n    weight = round(min_sd$weights, 2)\n  )"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#calculating-portfolio-returns",
    "href": "day2_part3_application_to_stock_portfolio.html#calculating-portfolio-returns",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Calculating Portfolio Returns",
    "text": "Calculating Portfolio Returns\nWe calculate the daily returns of the optimized portfolio by weighting the individual stock returns.\n\nportfolio_returns &lt;-\n  sp500_returns %&gt;%\n  left_join(weights, by = \"symbol\") %&gt;%\n  group_by(date) %&gt;%\n  summarise(\n    portfolio_return = sum(return * weight, na.rm = TRUE)\n  ) %&gt;%\n  mutate(portfolio_return_sq = portfolio_return^2)"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#exploratory-data-analysis-of-portfolio-returns",
    "href": "day2_part3_application_to_stock_portfolio.html#exploratory-data-analysis-of-portfolio-returns",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Exploratory Data Analysis of Portfolio Returns",
    "text": "Exploratory Data Analysis of Portfolio Returns\n\nPlotting Portfolio Returns\nWe visualize the portfolio returns and their squares over time to observe any patterns or volatility clustering.\n\nportfolio_returns %&gt;%\n  pivot_longer(starts_with(\"portfolio\")) %&gt;%\n  ggplot(aes(date, value)) +\n  geom_line() +\n  facet_wrap(~name, scales = \"free\", nrow = 2) +\n  labs(\n    title = \"Portfolio Returns and Squared Returns\",\n    x = \"Date\",\n    y = \"Value\"\n  )"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#time-series-modeling",
    "href": "day2_part3_application_to_stock_portfolio.html#time-series-modeling",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Time Series Modeling",
    "text": "Time Series Modeling\n\nChecking for Autocorrelation\nWe fit an ARIMA model to the portfolio returns to check for any autoregressive or moving average components.\n\nportfolio_returns %&gt;%\n  mutate(time = row_number()) %&gt;%\n  as_tsibble(index = time) %&gt;%\n  model(arima = fable::ARIMA(portfolio_return)) %&gt;% \n  print()\n\n# A mable: 1 x 1\n                   arima\n                 &lt;model&gt;\n1 &lt;ARIMA(2,0,2) w/ mean&gt;"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#garch-model-specification-and-fitting",
    "href": "day2_part3_application_to_stock_portfolio.html#garch-model-specification-and-fitting",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "GARCH Model Specification and Fitting",
    "text": "GARCH Model Specification and Fitting\n\nSpecifying the GARCH Model\nWe specify a Threshold GARCH (TGARCH) model to capture the asymmetric effects in volatility.\n\nspec &lt;-\n  ugarchspec(\n    variance.model = list(\n      model = \"fGARCH\",\n      submodel = \"TGARCH\",\n      garchOrder = c(1, 1)\n    ),\n    mean.model = list(\n      armaOrder = c(2, 2),\n      include.mean = TRUE\n    ),\n    distribution.model = \"std\"  # Student's t-distribution\n  )\n\n\nVariance Model: TGARCH(1,1) to capture asymmetry in volatility.\nMean Model: ARMA(2,2) to model the returns.\nDistribution: Student’s t-distribution to account for fat tails.\n\n\n\nFitting the GARCH Model\nWe fit the specified GARCH model to the portfolio returns.\n\nportfolio_fit &lt;-\n  ugarchfit(\n    spec = spec,\n    data = portfolio_returns$portfolio_return\n  )"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#model-evaluation",
    "href": "day2_part3_application_to_stock_portfolio.html#model-evaluation",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nComparing Actual vs. Fitted Returns\nWe compare the actual portfolio returns to the fitted values from the GARCH model.\n\ncomparison_data &lt;-\n  tibble(\n    date = portfolio_returns$date,\n    return_actual = portfolio_returns$portfolio_return,\n    return_fitted = fitted(portfolio_fit) %&gt;% as.numeric()\n  )\n\n\nPlotting the Comparison\n\ncomparison_data %&gt;%\n  pivot_longer(-date) %&gt;%\n  ggplot(aes(x = date, y = value, color = name)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    title = \"Actual vs. Fitted Portfolio Returns\",\n    y = \"Return\",\n    x = \"Date\",\n    color = \"Series\"\n  ) +\n  scale_color_brewer(\n    labels = c(\"Actual\", \"Fitted\"),\n    palette = \"Set1\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Volatility Estimates\nWe analyze the estimated volatility from the GARCH model and compare it to the absolute returns.\n\nvolatility_data &lt;-\n  tibble(\n    date = portfolio_returns$date,\n    return_abs = abs(portfolio_returns$portfolio_return),\n    sigma = sigma(portfolio_fit) %&gt;% as.numeric()\n  )\n\n\nPlotting Volatility Estimates\n\nvolatility_data %&gt;%\n  pivot_longer(\n    cols = c(return_abs, sigma),\n    names_to = \"measure\",\n    values_to = \"value\"\n  ) %&gt;%\n  ggplot(aes(x = date, y = value, color = measure)) +\n  geom_line() +\n  labs(\n    title = \"Estimated Volatility vs. Absolute Returns\",\n    y = \"Value\",\n    x = \"Date\",\n    color = \"Measure\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#rolling-forecast-and-var-analysis",
    "href": "day2_part3_application_to_stock_portfolio.html#rolling-forecast-and-var-analysis",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Rolling Forecast and VaR Analysis",
    "text": "Rolling Forecast and VaR Analysis\n\nPreparing Data for Rolling Forecast\nWe convert the portfolio returns data into an xts object required for the rolling forecast function.\n\ndf_portfolio_xts &lt;- xts(\n  x = portfolio_returns$portfolio_return,\n  order.by = portfolio_returns$date\n)\n\n\n\nPerforming Rolling Forecast with ugarchroll\nWe perform a rolling forecast to evaluate the model’s out-of-sample performance and calculate Value at Risk (VaR).\n\nroll_model &lt;-\n  ugarchroll(\n    spec = spec,\n    data = df_portfolio_xts,\n    n.ahead = 1,\n    forecast.length = 252, # Approximately one trading year\n    refit.every = 50,\n    refit.window = \"moving\",\n    calculate.VaR = TRUE,\n    VaR.alpha = c(0.01, 0.05)\n  )\nroll_model\n\n\n*-------------------------------------*\n*              GARCH Roll             *\n*-------------------------------------*\nNo.Refits       : 6\nRefit Horizon   : 50\nNo.Forecasts    : 252\nGARCH Model     : fGARCH(1,1)\n\nfGARCH SubModel : TGARCH\nDistribution    : std \n\nForecast Density:\n               Mu  Sigma Skew  Shape Shape(GIG) Realized\n2023-11-16 -1e-04 0.0041    0 7.6336          0   0.0000\n2023-11-17  3e-04 0.0038    0 7.6336          0   0.0001\n2023-11-20  2e-04 0.0035    0 7.6336          0   0.0006\n2023-11-21  4e-04 0.0033    0 7.6336          0   0.0023\n2023-11-22  2e-04 0.0032    0 7.6336          0   0.0043\n2023-11-24  2e-04 0.0032    0 7.6336          0   0.0026\n\n..........................\n              Mu  Sigma Skew  Shape Shape(GIG) Realized\n2024-11-08 7e-04 0.0047    0 7.3380          0   0.0055\n2024-11-11 0e+00 0.0046    0 7.3380          0   0.0001\n2024-11-12 6e-04 0.0042    0 7.3380          0  -0.0021\n2024-11-13 3e-04 0.0044    0 7.3380          0  -0.0002\n2024-11-14 6e-04 0.0040    0 7.7394          0  -0.0056\n2024-11-15 2e-04 0.0048    0 7.7394          0  -0.0038\n\nElapsed: 12.67342 secs\n\n\n\nn.ahead: Forecast horizon (1 day ahead).\nforecast.length: Number of periods to forecast (252 trading days).\nrefit.every: Refit the model every 50 observations.\nrefit.window: Use a moving window for refitting.\nVaR.alpha: Confidence levels for VaR calculation (1% and 5%).\n\n\n\nReporting VaR Results\nWe generate a report of the VaR backtesting results at the 1% significance level.\n\nreport(\n  roll_model,\n  type = \"VaR\",\n  VaR.alpha = 0.01,\n  conf.level = 0.95\n)\n\nVaR Backtest Report\n===========================================\nModel:              fGARCH-std\nBacktest Length:    252\nData:               \n\n==========================================\nalpha:              1%\nExpected Exceed:    2.5\nActual VaR Exceed:  2\nActual %:           0.8%\n\nUnconditional Coverage (Kupiec)\nNull-Hypothesis:    Correct Exceedances\nLR.uc Statistic:    0.117\nLR.uc Critical:     3.841\nLR.uc p-value:      0.733\nReject Null:        NO\n\nConditional Coverage (Christoffersen)\nNull-Hypothesis:    Correct Exceedances and\n                    Independence of Failures\nLR.cc Statistic:    0.149\nLR.cc Critical:     5.991\nLR.cc p-value:      0.928\nReject Null:        NO\n\n\n\n\nEvaluating Forecast Performance\nWe report the forecast performance measures (FPM) to assess the accuracy of the model’s forecasts.\n\nreport(roll_model, type = \"fpm\")\n\n\nGARCH Roll Mean Forecast Performance Measures\n---------------------------------------------\nModel       : fGARCH\nSubModel : TGARCH\nNo.Refits   : 6\nNo.Forecasts: 252\n\n        Stats\nMSE 0.0000181\nMAE 0.0032990\nDAC 0.5595000"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#visualizing-rolling-forecast-results",
    "href": "day2_part3_application_to_stock_portfolio.html#visualizing-rolling-forecast-results",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Visualizing Rolling Forecast Results",
    "text": "Visualizing Rolling Forecast Results\nWe plot various diagnostics from the rolling forecast to assess the model’s performance.\n\nConditional Density Forecasts\n\npar(mfrow = c(1, 1))\npar(new = FALSE) # Reset plotting parameters\nplot(roll_model, which = 1)\n\n\n\n\n\n\n\n\n\n\nActual vs. Forecasted Values (Mean)\n\npar(mfrow = c(1, 1))\npar(new = FALSE)\nplot(roll_model, which = 2)\n\n\n\n\n\n\n\n\n\n\nActual vs. Forecasted Values (Volatility)\n\npar(mfrow = c(1, 1))\npar(new = FALSE)\nplot(roll_model, which = 3)\n\n\n\n\n\n\n\n\n\n\nVaR Backtesting Plot (5% VaR)\n\npar(mfrow = c(1, 1))\npar(new = FALSE)\nplot(roll_model, which = 4)"
  },
  {
    "objectID": "day2_part3_application_to_stock_portfolio.html#conclusion",
    "href": "day2_part3_application_to_stock_portfolio.html#conclusion",
    "title": "Day 2 Part 3: Application to S&P 500 Stock Portfolio",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we:\n\nDownloaded and prepared S&P 500 stock data.\nConstructed a minimum variance portfolio using portfolio optimization techniques.\nFitted a TGARCH model to the portfolio returns to model volatility.\nEvaluated the model’s fit and compared actual vs. fitted returns.\nPerformed a rolling forecast to assess the model’s out-of-sample performance.\nCalculated and analyzed Value at Risk (VaR) for risk assessment.\n\nThis practical application demonstrates the integration of portfolio optimization and time series econometric modeling for financial analysis and risk management."
  },
  {
    "objectID": "day2_part1_volatility_processes.html",
    "href": "day2_part1_volatility_processes.html",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "",
    "text": "In this session, we will simulate various volatility processes using different GARCH models. By specifying and simulating data from these models, we aim to understand how they capture the dynamics of financial time series, particularly the behavior of volatility over time."
  },
  {
    "objectID": "day2_part1_volatility_processes.html#introduction",
    "href": "day2_part1_volatility_processes.html#introduction",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "",
    "text": "In this session, we will simulate various volatility processes using different GARCH models. By specifying and simulating data from these models, we aim to understand how they capture the dynamics of financial time series, particularly the behavior of volatility over time."
  },
  {
    "objectID": "day2_part1_volatility_processes.html#loading-required-libraries",
    "href": "day2_part1_volatility_processes.html#loading-required-libraries",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\nFirst, we load the necessary R packages that will be used throughout this analysis.\n\nlibrary(tidyverse)         # Data manipulation and visualization\nlibrary(tidyfinance)       # Financial data tools\nlibrary(tsibble)           # Time series data manipulation\nlibrary(feasts)            # Time series analysis tools\nlibrary(tseries)           # Time series analysis\nlibrary(broom)             # Tidying statistical outputs\nlibrary(PortfolioAnalytics) # Portfolio optimization\nlibrary(rugarch)           # GARCH models"
  },
  {
    "objectID": "day2_part1_volatility_processes.html#setting-simulation-parameters",
    "href": "day2_part1_volatility_processes.html#setting-simulation-parameters",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Setting Simulation Parameters",
    "text": "Setting Simulation Parameters\nWe set the number of observations for our simulations.\n\nn_sim &lt;- 1000  # Number of time periods to simulate"
  },
  {
    "objectID": "day2_part1_volatility_processes.html#specifying-garch-models",
    "href": "day2_part1_volatility_processes.html#specifying-garch-models",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Specifying GARCH Models",
    "text": "Specifying GARCH Models\nWe will specify four different GARCH models:\n\nStandard GARCH (sGARCH)\nGJR-GARCH (gjrGARCH)\nExponential GARCH (eGARCH)\nAsymmetric Power ARCH (apARCH)\n\n\n1. Standard GARCH Model\nThe standard GARCH model, as proposed by Bollerslev (1986), can be mathematically represented as follows:\n\n\\sigma_t^2 = \\omega + \\sum_{j=1}^q \\alpha_j \\epsilon_{t-j}^2 + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^2\n\nWhere:\n\n\\sigma_t^2: Conditional variance at time t\n\\omega: Intercept term\n\\alpha_j: Coefficients for lagged error terms (ARCH terms)\n\\beta_j: Coefficients for lagged variance terms (GARCH terms)\n\nThis model captures volatility clustering, a characteristic often observed in financial time series, where periods of high volatility tend to be followed by more high volatility. We specify a standard GARCH(1,1) model using the ugarchspec function from the rugarch package. This model includes an AR(1) term in the mean equation and GARCH(1,1) terms in the variance equation.\n\ns_garch &lt;-\n  ugarchspec(\n      variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n      mean.model = list(armaOrder = c(1, 0)),\n      fixed.pars =\n        c(\n          mu = 0.1,      # Intercept of the mean equation\n          ar1 = 0.9,     # AR(1) coefficient\n          omega = 0.1,   # Intercept of the variance equation\n          alpha1 = 0.7,  # ARCH coefficient\n          beta1 = 0.1    # GARCH coefficient\n          ),\n      distribution.model = \"norm\"\n    )\ns_garch\n\n\n*---------------------------------*\n*       GARCH Model Spec          *\n*---------------------------------*\n\nConditional Variance Dynamics   \n------------------------------------\nGARCH Model     : sGARCH(1,1)\nVariance Targeting  : FALSE \n\nConditional Mean Dynamics\n------------------------------------\nMean Model      : ARFIMA(1,0,0)\nInclude Mean        : TRUE \nGARCH-in-Mean       : FALSE \n\nConditional Distribution\n------------------------------------\nDistribution    :  norm \nIncludes Skew   :  FALSE \nIncludes Shape  :  FALSE \nIncludes Lambda :  FALSE \n\n\n\n\n2. GJR-GARCH Model\nThe GJR-GARCH model, introduced by Glosten, Jagannathan, and Runkle (1993), extends the standard GARCH model by allowing for asymmetry in the volatility response to positive and negative shocks. The model is represented by:\n\n\\sigma_t^2 = \\omega + \\sum_{j=1}^q \\left( \\alpha_j \\epsilon_{t-j}^2 + \\gamma_j I(\\epsilon_{t-j} &lt; 0) \\epsilon_{t-j}^2 \\right) + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^2\n\nWhere:\n\n\\gamma_j: Represents the leverage effect, capturing the impact of negative shocks\nI(\\epsilon_{t-j} &lt; 0): Indicator function that equals 1 if the lagged error term is negative, otherwise 0\n\nThis model allows volatility to react differently to positive and negative shocks, which is often observed in financial markets (e.g., bad news tends to increase volatility more than good news). We specify the GJR-GARCH model as follows:\n\ngjr_garch &lt;-\n  ugarchspec(\n      variance.model = list(model = \"gjrGARCH\", garchOrder = c(1, 1)),\n      mean.model = list(armaOrder = c(1, 0)),\n      fixed.pars =\n        c(\n          mu = 0.1,       # Intercept of the mean equation\n          ar1 = 0.9,      # AR(1) coefficient\n          omega = 0.1,    # Intercept of the variance equation\n          alpha1 = 0.5,   # ARCH(1) coefficient\n          beta1 = 0.1,    # GARCH(1) coefficient\n          gamma1 = 0.1    # Asymmetry term\n          ),\n      distribution.model = \"norm\"\n    )\ngjr_garch\n\n\n*---------------------------------*\n*       GARCH Model Spec          *\n*---------------------------------*\n\nConditional Variance Dynamics   \n------------------------------------\nGARCH Model     : gjrGARCH(1,1)\nVariance Targeting  : FALSE \n\nConditional Mean Dynamics\n------------------------------------\nMean Model      : ARFIMA(1,0,0)\nInclude Mean        : TRUE \nGARCH-in-Mean       : FALSE \n\nConditional Distribution\n------------------------------------\nDistribution    :  norm \nIncludes Skew   :  FALSE \nIncludes Shape  :  FALSE \nIncludes Lambda :  FALSE \n\n\n\n\n3. Exponential GARCH Model\nThe Exponential GARCH (eGARCH) model, proposed by Nelson (1991), captures leverage effects by modeling the logarithm of the conditional variance:\n\n\\log(\\sigma_t^2) = \\omega + \\sum_{j=1}^q \\left( \\alpha_j z_{t-j} + \\gamma_j (|z_{t-j}| - E|z_{t-j}|) \\right) + \\sum_{j=1}^p \\beta_j \\log(\\sigma_{t-j}^2)\n\nWhere:\n\nz_t: Standardized residuals\n\\alpha_j: Captures the sign effect (impact of positive/negative shocks)\n\\gamma_j: Captures the size effect (impact of the magnitude of shocks)\n\nThis model naturally ensures positive variance without requiring parameter restrictions and effectively models asymmetric volatility. We specify the eGARCH model as follows:\n\ne_garch &lt;-\n  ugarchspec(\n      variance.model = list(model = \"eGARCH\", garchOrder = c(1, 1)),\n      mean.model = list(armaOrder = c(1, 0)),\n      fixed.pars =\n        c(\n          mu = 0.1,       # Intercept of the mean equation\n          ar1 = 0.9,      # AR(1) coefficient\n          omega = 0.1,    # Intercept of the variance equation\n          alpha1 = 0.5,   # ARCH(1) coefficient\n          beta1 = 0.4,    # GARCH(1) coefficient\n          gamma1 = 0.1    # Leverage term\n          ),\n      distribution.model = \"norm\"\n    )\ne_garch\n\n\n*---------------------------------*\n*       GARCH Model Spec          *\n*---------------------------------*\n\nConditional Variance Dynamics   \n------------------------------------\nGARCH Model     : eGARCH(1,1)\nVariance Targeting  : FALSE \n\nConditional Mean Dynamics\n------------------------------------\nMean Model      : ARFIMA(1,0,0)\nInclude Mean        : TRUE \nGARCH-in-Mean       : FALSE \n\nConditional Distribution\n------------------------------------\nDistribution    :  norm \nIncludes Skew   :  FALSE \nIncludes Shape  :  FALSE \nIncludes Lambda :  FALSE \n\n\n\n\n4. Asymmetric Power ARCH Model\nThe Asymmetric Power ARCH (apARCH) model, introduced by Ding, Granger, and Engle (1993), allows for both leverage and power effects in the volatility equation:\n\n\\sigma_t^\\delta = \\omega + \\sum_{j=1}^q \\alpha_j (|\\epsilon_{t-j}| - \\gamma_j \\epsilon_{t-j})^\\delta + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^\\delta\n\nWhere:\n\n\\delta: Power parameter, allowing for flexibility in modeling the distribution of volatility\n\\gamma_j: Leverage term, similar to the GJR-GARCH model\n\nThis model captures the observed behavior where volatility can respond asymmetrically to past shocks and allows for different power transformations of volatility. We specify the apARCH model as follows:\n\nap_arch &lt;-\n  ugarchspec(\n      variance.model = list(model = \"apARCH\", garchOrder = c(1, 1)),\n      mean.model = list(armaOrder = c(1, 0)),\n      fixed.pars =\n        c(\n          mu = 0.1,       # Intercept of the mean equation\n          ar1 = 0.9,      # AR(1) coefficient\n          omega = 0.1,    # Intercept of the variance equation\n          alpha1 = 0.5,   # ARCH(1) coefficient\n          beta1 = 0.4,    # GARCH(1) coefficient\n          gamma1 = 0.1,   # Asymmetry term\n          delta = 1       # Power parameter\n          ),\n      distribution.model = \"norm\"\n    )\nap_arch\n\n\n*---------------------------------*\n*       GARCH Model Spec          *\n*---------------------------------*\n\nConditional Variance Dynamics   \n------------------------------------\nGARCH Model     : apARCH(1,1)\nVariance Targeting  : FALSE \n\nConditional Mean Dynamics\n------------------------------------\nMean Model      : ARFIMA(1,0,0)\nInclude Mean        : TRUE \nGARCH-in-Mean       : FALSE \n\nConditional Distribution\n------------------------------------\nDistribution    :  norm \nIncludes Skew   :  FALSE \nIncludes Shape  :  FALSE \nIncludes Lambda :  FALSE"
  },
  {
    "objectID": "day2_part1_volatility_processes.html#compiling-the-model-specifications",
    "href": "day2_part1_volatility_processes.html#compiling-the-model-specifications",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Compiling the Model Specifications",
    "text": "Compiling the Model Specifications\nWe compile all the model specifications into a list for easy reference and manipulation.\n\nvolatility_specs &lt;-\n  list(\n    s_garch = s_garch,\n    gjr_garch = gjr_garch,\n    e_garch = e_garch,\n    ap_arch = ap_arch\n  )"
  },
  {
    "objectID": "day2_part1_volatility_processes.html#simulating-data-from-the-garch-models",
    "href": "day2_part1_volatility_processes.html#simulating-data-from-the-garch-models",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Simulating Data from the GARCH Models",
    "text": "Simulating Data from the GARCH Models\nWe simulate time series data from each of the specified GARCH models and combine the results into a single data frame.\n\nsim_data &lt;-\n  map(\n    names(volatility_specs),\n    function(model_name) {\n      spec &lt;- volatility_specs[[model_name]]\n      simulation &lt;- ugarchpath(spec = spec, n.sim = n_sim)\n      tibble(\n        time = 1:n_sim,\n        series = as.numeric(simulation@path$seriesSim),\n        sigma = as.numeric(simulation@path$sigmaSim),\n        model = model_name\n      )\n    }\n  ) %&gt;%\n  list_rbind()\nsim_data\n\n# A tibble: 4,000 × 4\n    time  series sigma model  \n   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1     1  0.0549 0.387 s_garch\n 2     2  0.144  0.341 s_garch\n 3     3 -0.319  0.341 s_garch\n 4     4 -1.08   0.509 s_garch\n 5     5 -1.74   0.760 s_garch\n 6     6 -0.459  0.765 s_garch\n 7     7 -0.120  1.00  s_garch\n 8     8  0.651  0.507 s_garch\n 9     9  0.363  0.720 s_garch\n10    10 -0.197  0.436 s_garch\n# ℹ 3,990 more rows"
  },
  {
    "objectID": "day2_part1_volatility_processes.html#visualizing-the-simulated-data",
    "href": "day2_part1_volatility_processes.html#visualizing-the-simulated-data",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Visualizing the Simulated Data",
    "text": "Visualizing the Simulated Data\nWe visualize the simulated series and their corresponding volatilities for each model.\n\nsim_data %&gt;%\n  pivot_longer(c(series, sigma)) %&gt;%\n  ggplot(aes(time, value, color = model)) +\n  geom_line() +\n  facet_wrap(~name, scales = \"free\", ncol=2) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure: Simulated time series and volatility from different GARCH models."
  },
  {
    "objectID": "day2_part1_volatility_processes.html#analyzing-the-volatility-processes",
    "href": "day2_part1_volatility_processes.html#analyzing-the-volatility-processes",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Analyzing the Volatility Processes",
    "text": "Analyzing the Volatility Processes\nTo further understand the volatility processes generated by each model, we will examine the time series properties of the volatility series.\n\nDefining a Plotting Function\nWe define a function plot_tsdisplay that creates time series plots along with the autocorrelation function (ACF) and partial autocorrelation function (PACF) for the volatility series.\n\nplot_tsdisplay &lt;-\n  function(model_name) {\n    sim_data %&gt;%\n      filter(model == model_name) %&gt;%\n      as_tsibble(index = time) %&gt;%\n      gg_tsdisplay(sigma, plot_type = \"partial\")\n  }\n\n\n\nAnalyzing Each Model\n\nStandard GARCH Model\n\nplot_tsdisplay(\"s_garch\")\n\n\n\n\n\n\n\n\n\n\nGJR-GARCH Model\n\nplot_tsdisplay(\"gjr_garch\")\n\n\n\n\n\n\n\n\n\n\nExponential GARCH Model\n\nplot_tsdisplay(\"e_garch\")\n\n\n\n\n\n\n\n\n\n\nAsymmetric Power ARCH Model\n\nplot_tsdisplay(\"ap_arch\")\n\n\n\n\n\n\n\n\nFigures: Time series, ACF, and PACF plots of the volatility series for each GARCH model."
  },
  {
    "objectID": "day2_part1_volatility_processes.html#conclusion",
    "href": "day2_part1_volatility_processes.html#conclusion",
    "title": "Day 2 Part 1: Simulating Volatility Processes",
    "section": "Conclusion",
    "text": "Conclusion\nIn this exercise, we simulated and analyzed different GARCH models to understand how they capture volatility in time series data. By comparing the simulated series and their volatility processes, we observed the characteristics and differences among the models. This hands-on approach enhances our understanding of volatility modeling in financial econometrics."
  },
  {
    "objectID": "day1_part3_random_walks.html",
    "href": "day1_part3_random_walks.html",
    "title": "Day 1 Part 3: Random Walks",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries will enable us to generate, visualize, and analyze random walks, which are fundamental to understanding stochastic processes in time series econometrics.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)\nlibrary(fixest)"
  },
  {
    "objectID": "day1_part3_random_walks.html#load-libraries",
    "href": "day1_part3_random_walks.html#load-libraries",
    "title": "Day 1 Part 3: Random Walks",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries will enable us to generate, visualize, and analyze random walks, which are fundamental to understanding stochastic processes in time series econometrics.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)\nlibrary(fixest)"
  },
  {
    "objectID": "day1_part3_random_walks.html#illustrating-a-random-walk",
    "href": "day1_part3_random_walks.html#illustrating-a-random-walk",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Illustrating a Random Walk",
    "text": "Illustrating a Random Walk\nGenerate and visualize a single random walk. A random walk is a simple but important stochastic process used in time series analysis to model variables whose changes are entirely unpredictable. Here, we simulate a single random walk to understand its behavior.\nMathematically, a random walk can be represented as:\n y_t = y_{t-1} + e_t \nwhere e_t is white noise, typically assumed to be normally distributed with mean zero and constant variance. The difference between consecutive values, y_t - y_{t-1} = e_t, is white noise, which implies that the changes in a random walk are completely random and unpredictable.\n\nsteps &lt;- 100\ne &lt;- rnorm(steps)\ny &lt;- e\ncoef &lt;- 1\ndrift = 0\n\nplot(e, type=\"l\")\n\n\n\n\n\n\n\nfor (i in 2:steps) {\n  y[i] &lt;- drift + coef*y[i-1] + e[i]\n}\n\nplot(y, type=\"l\")"
  },
  {
    "objectID": "day1_part3_random_walks.html#multiple-random-walks",
    "href": "day1_part3_random_walks.html#multiple-random-walks",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Multiple Random Walks",
    "text": "Multiple Random Walks\nGenerate and visualize multiple random walks. Generating multiple random walks allows us to explore the variability between different realizations of the process, illustrating the concept of stochasticity and highlighting the random nature of these paths.\n\nn_random_walks &lt;- 30\n\ncreate_random_walk &lt;- \n  function(random_walk_no, steps = 100, n_random_walks = 30) {\n    \n    innovations &lt;- \n      tibble(\n        random_walk_no, \n        step = 1:steps,\n        e = rnorm(steps)\n      ) \n    \n    random_walk &lt;- \n      innovations %&gt;% \n        mutate(random_walk = cumsum(e))\n    \n    return(random_walk)\n  }\n\nrandom_walks &lt;- \n  1:n_random_walks %&gt;%  \n    map(~create_random_walk(.)) %&gt;% \n    list_rbind()\n\nrandom_walks %&gt;% \n  mutate(random_walk_no = factor(random_walk_no, 1:n_random_walks)) %&gt;% \n  ggplot(aes(x = step, y = random_walk, color = random_walk_no)) +\n  geom_line()"
  },
  {
    "objectID": "day1_part3_random_walks.html#analysis-of-a-single-random-walk",
    "href": "day1_part3_random_walks.html#analysis-of-a-single-random-walk",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Analysis of a Single Random Walk",
    "text": "Analysis of a Single Random Walk\nAnalyze a single random walk with a lag variable. Here, we analyze the properties of a single random walk by estimating a regression with a lagged variable. This helps us understand if past values of the walk can predict future values, which is key to determining whether the series is truly a random walk.\n\nrandom_walk_1 &lt;- \n  random_walks %&gt;% \n  filter(random_walk_no == 1) %&gt;% \n  mutate(random_walk_lag = lag(random_walk)) %&gt;% \n  remove_missing() %&gt;% \n  print()\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange.\n\n\n# A tibble: 99 × 5\n   random_walk_no  step       e random_walk random_walk_lag\n            &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1              1     2 -0.395      -1.06           -0.667 \n 2              1     3  0.206      -0.855          -1.06  \n 3              1     4  1.15        0.299          -0.855 \n 4              1     5 -0.0244      0.275           0.299 \n 5              1     6 -1.07       -0.794           0.275 \n 6              1     7  0.806       0.0124         -0.794 \n 7              1     8 -0.516      -0.504           0.0124\n 8              1     9  0.414      -0.0899         -0.504 \n 9              1    10  0.128       0.0379         -0.0899\n10              1    11  0.297       0.335           0.0379\n# ℹ 89 more rows\n\n\nNext, we estimate the following regression equation:\n y_t = \\beta y_{t-1} + \\epsilon_t \nwhere y_t is the value of the random walk at time t, y_{t-1} is the lagged value at time t-1, \\beta is the estimated coefficient, and \\epsilon_t is the residual (error term). In this case, there is no intercept term, which is why the regression formula includes -1 to remove the intercept.\n\nest &lt;- feols(random_walk ~ random_walk_lag - 1, data = random_walk_1)\netable(est)\n\n                              est\nDependent Var.:       random_walk\n                                 \nrandom_walk_lag 1.013*** (0.0228)\n_______________ _________________\nS.E. type                     IID\nObservations                   99\nR2                        0.88212\nAdj. R2                   0.88212\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that the estimate of \\beta is less than one due to the finite sample bias associated with unit root processes. This bias, often called the “small sample bias,” arises because of the non-stationary nature of a unit root process, causing the estimated \\beta to be slightly lower than its true value of 1 in most realizations."
  },
  {
    "objectID": "day1_part3_random_walks.html#monte-carlo-simulation-of-random-walks",
    "href": "day1_part3_random_walks.html#monte-carlo-simulation-of-random-walks",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Monte Carlo Simulation of Random Walks",
    "text": "Monte Carlo Simulation of Random Walks\nSimulate a large number of random walks and estimate coefficients. Monte Carlo simulations allow us to repeatedly generate random walks and estimate their coefficients to understand the distribution of these coefficients. This is important for understanding the statistical properties of random walks and verifying theoretical expectations.\n\nrandom_walks &lt;- \n  1:10^3 %&gt;%  \n  map(~create_random_walk(.)) %&gt;% \n  list_rbind() %&gt;% \n  group_by(random_walk_no) %&gt;% \n  mutate(random_walk_lag = lag(random_walk)) %&gt;% \n  remove_missing() %&gt;% \n  nest() %&gt;% \n  mutate(\n    est = map(data, ~feols(random_walk ~ random_walk_lag - 1, data = .)),\n    coef = map(est, ~.$coefficients)\n  ) %&gt;% \n  unnest(cols = coef) %&gt;% \n  ungroup()\n\nWarning: Removed 1000 rows containing missing values or values outside the\nscale range.\n\nrandom_walks\n\n# A tibble: 1,000 × 4\n   random_walk_no data              est       coef\n            &lt;int&gt; &lt;list&gt;            &lt;list&gt;   &lt;dbl&gt;\n 1              1 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.930\n 2              2 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 1.01 \n 3              3 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.895\n 4              4 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.971\n 5              5 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.985\n 6              6 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.995\n 7              7 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.990\n 8              8 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.979\n 9              9 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 0.973\n10             10 &lt;tibble [99 × 4]&gt; &lt;fixest&gt; 1.01 \n# ℹ 990 more rows"
  },
  {
    "objectID": "day1_part3_random_walks.html#summary-of-coefficients",
    "href": "day1_part3_random_walks.html#summary-of-coefficients",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Summary of Coefficients",
    "text": "Summary of Coefficients\nSummarize and visualize the distribution of estimated coefficients. Summarizing the coefficients from the Monte Carlo simulations allows us to visualize the typical behavior of the estimated relationships in random walks, such as their average and variability. This can help us draw conclusions about the nature of randomness and persistence in these series.\n\ncoef_mean &lt;- random_walks %&gt;% summarise(coef_mean = mean(coef)) %&gt;% pull(coef_mean)\n\nrandom_walks %&gt;% \n  ggplot(aes(coef)) + \n  geom_density() + \n  geom_vline(xintercept = coef_mean, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 1, linetype = \"twodash\", color = \"navy\") +\n  scale_x_continuous(\n    breaks = c(coef_mean, 1),\n    labels = c(\"coef_mean\" = \"mean estimate\", \"1\" = \"1 (true)\")\n  )"
  },
  {
    "objectID": "day1_part3_random_walks.html#exercises",
    "href": "day1_part3_random_walks.html#exercises",
    "title": "Day 1 Part 3: Random Walks",
    "section": "Exercises",
    "text": "Exercises\nAdapt the following code for the process\ny_t = \\alpha + \\beta y_{t-1} + \\epsilon_{t}\nand plot the process for each of these cases. Pick your own values for \\alpha and \\beta that satisfy these cases.\n\nSet \\alpha&gt;0 and \\beta=1\nSet \\alpha&lt;0 and \\beta=1\nSet \\alpha&lt;0 and -1 &lt; \\beta &lt; 1\nSet \\alpha=0 and \\beta&gt;1\n\n\nn_steps &lt;- 100\nepsilon &lt;- rnorm(n_steps)\ny &lt;- rep(0, n_steps)\nbeta &lt;- 1\nalpha &lt;- 0\n\nfor (i in 2:steps) {\n  y[i] &lt;- alpha + beta*y[i-1] + epsilon[i]\n}\n\nplot(y, type=\"l\")\n\nProcess 3. is “explosive” and 4. is “convergent”. From the plots you have created, note down observations why this is the case."
  },
  {
    "objectID": "day1_part2_time_series_data.html",
    "href": "day1_part2_time_series_data.html",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These packages will enable sophisticated econometric analysis, such as stationarity checks, model fitting, and forecasting of financial time series data. These libraries will allow us to efficiently handle and visualize time series data, perform statistical analysis, and work with financial datasets.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)"
  },
  {
    "objectID": "day1_part2_time_series_data.html#load-libraries",
    "href": "day1_part2_time_series_data.html#load-libraries",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These packages will enable sophisticated econometric analysis, such as stationarity checks, model fitting, and forecasting of financial time series data. These libraries will allow us to efficiently handle and visualize time series data, perform statistical analysis, and work with financial datasets.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)"
  },
  {
    "objectID": "day1_part2_time_series_data.html#download-apple-stock-data",
    "href": "day1_part2_time_series_data.html#download-apple-stock-data",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Download Apple Stock Data",
    "text": "Download Apple Stock Data\nDownload Apple stock prices from 2010 to 2020 and inspect the data. This time series will be used to explore stock price dynamics, perform volatility analysis, and conduct model estimation for financial econometric purposes. This data will help us understand Apple’s historical performance and serve as the foundation for our time-series analysis.\n\nAAPL &lt;- \n  download_data(\n    \"stock_prices\", \n    symbols = \"AAPL\", \n    start = \"2010-01-01\", \n    end = \"2020-01-01\"\n  )\n\nAAPL %&gt;% glimpse()\n\nRows: 2,516\nColumns: 8\n$ symbol         &lt;chr&gt; \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\",…\n$ date           &lt;date&gt; 2010-01-04, 2010-01-05, 2010-01-06, 2010-01-07, 2010-0…\n$ volume         &lt;dbl&gt; 493729600, 601904800, 552160000, 477131200, 447610800, …\n$ open           &lt;dbl&gt; 7.622500, 7.664286, 7.656429, 7.562500, 7.510714, 7.600…\n$ low            &lt;dbl&gt; 7.585000, 7.616071, 7.526786, 7.466071, 7.466429, 7.444…\n$ high           &lt;dbl&gt; 7.660714, 7.699643, 7.686786, 7.571429, 7.571429, 7.607…\n$ close          &lt;dbl&gt; 7.643214, 7.656429, 7.534643, 7.520714, 7.570714, 7.503…\n$ adjusted_close &lt;dbl&gt; 6.447412, 6.458561, 6.355827, 6.344078, 6.386256, 6.329…"
  },
  {
    "objectID": "day1_part2_time_series_data.html#prepare-closing-price-data",
    "href": "day1_part2_time_series_data.html#prepare-closing-price-data",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Prepare Closing Price Data",
    "text": "Prepare Closing Price Data\nRename and organize closing prices, converting the data into a tsibble. The tsibble structure will allow for the application of time series econometric methods, such as differentiation and model specification, by leveraging its temporal index. A tsibble is a specialized time-series data format in R, which allows us to perform various time-based operations more easily.\n\nclosing_price &lt;- \n  AAPL %&gt;% \n  rename(price = adjusted_close) %&gt;% \n  select(symbol, date, price) %&gt;% \n  as_tsibble(\n    index = date, \n    regular = FALSE\n  ) %&gt;% \n  glimpse()\n\nRows: 2,516\nColumns: 3\n$ symbol &lt;chr&gt; \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\",…\n$ date   &lt;date&gt; 2010-01-04, 2010-01-05, 2010-01-06, 2010-01-07, 2010-01-08, 20…\n$ price  &lt;dbl&gt; 6.447412, 6.458561, 6.355827, 6.344078, 6.386256, 6.329917, 6.2…"
  },
  {
    "objectID": "day1_part2_time_series_data.html#calculate-logarithmic-returns",
    "href": "day1_part2_time_series_data.html#calculate-logarithmic-returns",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Calculate Logarithmic Returns",
    "text": "Calculate Logarithmic Returns\nCalculate the log of prices and the daily log returns. Logarithmic returns are useful for time series econometrics because they stabilize variance over time and allow additive decomposition of returns, simplifying the model estimation and analysis. Log returns are commonly used in finance to better capture the relative change in prices and handle the compounding nature of returns.\n\nlog_returns &lt;- \n  closing_price %&gt;% \n  mutate(\n    lprice = log(price),\n    lreturn = difference(lprice, lag = 1, differences = 1)\n  ) %&gt;% \n  glimpse()\n\nRows: 2,516\nColumns: 5\n$ symbol  &lt;chr&gt; \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\"…\n$ date    &lt;date&gt; 2010-01-04, 2010-01-05, 2010-01-06, 2010-01-07, 2010-01-08, 2…\n$ price   &lt;dbl&gt; 6.447412, 6.458561, 6.355827, 6.344078, 6.386256, 6.329917, 6.…\n$ lprice  &lt;dbl&gt; 1.863679, 1.865407, 1.849372, 1.847522, 1.854148, 1.845287, 1.…\n$ lreturn &lt;dbl&gt; NA, 0.001727791, -0.016034522, -0.001850293, 0.006626426, -0.0…"
  },
  {
    "objectID": "day1_part2_time_series_data.html#visualize-prices-and-returns",
    "href": "day1_part2_time_series_data.html#visualize-prices-and-returns",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Visualize Prices and Returns",
    "text": "Visualize Prices and Returns\n\nPrice Over Time\nThis plot will show the trend of Apple’s stock price over the given period, providing an overview of its growth or decline. This visualization is crucial for identifying non-stationarity, potential structural breaks, and trends that may necessitate differencing before model fitting.\n\nlog_returns %&gt;% autoplot(.vars = price)\n\n\n\n\n\n\n\n\n\n\nLog Price Over Time\nThe log price plot allows us to visualize the price changes on a logarithmic scale, which is useful for observing relative growth. This transformation helps in linearizing exponential growth patterns, making it more appropriate for econometric modeling and reducing potential heteroskedasticity.\n\nlog_returns %&gt;% autoplot(.vars = lprice)\n\n\n\n\n\n\n\n\n\n\nLog Returns Over Time\nThis plot will depict the daily log returns, highlighting the variability and volatility of Apple’s stock over time. Examining log returns over time can reveal volatility clustering, a common feature in financial time series that will inform our choice of econometric models, such as GARCH.\n\nlog_returns %&gt;% autoplot(.vars = lreturn)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "day1_part2_time_series_data.html#quantile-analysis",
    "href": "day1_part2_time_series_data.html#quantile-analysis",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Quantile Analysis",
    "text": "Quantile Analysis\nCalculate the 5th percentile of daily log returns. The 5th percentile is often used in Value at Risk (VaR) calculations to assess the potential downside risk in holding the asset over a given time frame. The 5th percentile helps us understand the lower tail of the return distribution, indicating extreme negative returns that could represent risk scenarios.\n\nquantile_05 &lt;- \n  quantile(\n    log_returns %&gt;% \n      remove_missing() %&gt;% \n      pull(lreturn), \n    probs = 0.05\n  )\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange.\n\nquantile_05\n\n         5% \n-0.02531911"
  },
  {
    "objectID": "day1_part2_time_series_data.html#plot-distribution-of-daily-returns",
    "href": "day1_part2_time_series_data.html#plot-distribution-of-daily-returns",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Plot Distribution of Daily Returns",
    "text": "Plot Distribution of Daily Returns\nPlot a histogram of daily log returns with a dashed line for the 5th percentile. Understanding the distribution of returns is critical for time series modeling, as it informs the assumptions of normality or fat tails, which are essential for selecting appropriate econometric models. The histogram gives us a visual representation of the return distribution, while the dashed line indicates the 5th percentile, helping us identify the risk threshold.\n\nlog_returns %&gt;% \n  ggplot(aes(x = lreturn)) +\n  geom_histogram(bins = 100) +\n  geom_vline(aes(xintercept = quantile_05),\n             linetype = \"dashed\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of daily Apple stock returns\"\n  ) +\n  scale_x_continuous(labels = percent)\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "day1_part2_time_series_data.html#aggregate-weekly-log-returns",
    "href": "day1_part2_time_series_data.html#aggregate-weekly-log-returns",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Aggregate Weekly Log Returns",
    "text": "Aggregate Weekly Log Returns\nSummarize log returns by week and plot the results. Aggregating returns to a weekly level can help mitigate the noise inherent in daily data and better reveal underlying cyclical patterns, which can be relevant for macroeconomic linkages. Aggregating returns by week helps smooth out daily fluctuations and provides a clearer picture of the overall trend.\n\nlog_returns_weekly &lt;- \n  log_returns %&gt;% \n    index_by(yearweek = ~yearweek(.)) %&gt;% \n    summarise(lreturn = sum(lreturn)) %&gt;% \n    glimpse()\n\nRows: 522\nColumns: 2\n$ yearweek &lt;week&gt; 2010 W01, 2010 W02, 2010 W03, 2010 W04, 2010 W05, 2010 W06, …\n$ lreturn  &lt;dbl&gt; NA, -0.028955741, -0.040532669, -0.029195736, 0.017547702, 0.…\n\nlog_returns_weekly %&gt;% autoplot()\n\nPlot variable not specified, automatically selected `.vars = lreturn`\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "day1_part2_time_series_data.html#aggregate-monthly-log-returns",
    "href": "day1_part2_time_series_data.html#aggregate-monthly-log-returns",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Aggregate Monthly Log Returns",
    "text": "Aggregate Monthly Log Returns\nSummarize log returns by month and plot the results. Monthly aggregation is useful for analyzing longer-term trends and understanding seasonality, which are key considerations in time series econometric models, such as ARIMA. Monthly aggregation allows us to observe long-term patterns and seasonal trends in Apple’s stock performance.\n\nlog_returns_monthly &lt;- \n  log_returns %&gt;% \n    index_by(yearmonth = ~yearmonth(.)) %&gt;% \n    summarise(lreturn = sum(lreturn)) %&gt;% \n    glimpse()\n\nRows: 120\nColumns: 2\n$ yearmonth &lt;mth&gt; 2010 Jan, 2010 Feb, 2010 Mar, 2010 Apr, 2010 May, 2010 Jun, …\n$ lreturn   &lt;dbl&gt; NA, 0.063346631, 0.138430335, 0.105280303, -0.016255829, -0.…\n\nlog_returns_monthly %&gt;% autoplot()\n\nPlot variable not specified, automatically selected `.vars = lreturn`\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "day1_part2_time_series_data.html#exercises",
    "href": "day1_part2_time_series_data.html#exercises",
    "title": "Day 1 Part 2: Exploring Time Series Data",
    "section": "Exercises",
    "text": "Exercises\nIn this exercise you will download data for a stock of your choice and apply the concepts you have learned to its data.\n\nRead the documentation of the data_download function using the command ?download_data and download the constituents from one of the supported indices: DAX, EURO STOXX 50, Dow Jones Industrial Average, Russell 1000, Russell 2000, Russell 3000, S&P 100, S&P 500, Nasdaq 100, FTSE 100, MSCI World, Nikkei 225, TOPIX.\nPick one of the constituents, look it up on Yahoo Finance, download its data using data_download and plot the adjusted closing price.\nAggregate the series to the monthly level and plot the monthly log returns."
  },
  {
    "objectID": "downloads.html",
    "href": "downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Script files: day1_code.zip\nWorkbooks: day1_workbooks.zip"
  },
  {
    "objectID": "downloads.html#day-1",
    "href": "downloads.html#day-1",
    "title": "Downloads",
    "section": "",
    "text": "Script files: day1_code.zip\nWorkbooks: day1_workbooks.zip"
  },
  {
    "objectID": "downloads.html#day-2",
    "href": "downloads.html#day-2",
    "title": "Downloads",
    "section": "Day 2",
    "text": "Day 2\nScript files: day2_code.zip\nWorkbooks: day2_workbooks.zip"
  },
  {
    "objectID": "day2_part2_fitting_models.html",
    "href": "day2_part2_fitting_models.html",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "",
    "text": "In this session, we will focus on fitting various volatility models to financial time series data, specifically the S&P 500 index. We will:\n\nDownload and prepare the S&P 500 data.\nPerform exploratory data analysis (EDA).\nFit different GARCH models.\nEvaluate model diagnostics.\nForecast future volatility.\nCalculate Value at Risk (VaR) and Expected Shortfall (ES)."
  },
  {
    "objectID": "day2_part2_fitting_models.html#introduction",
    "href": "day2_part2_fitting_models.html#introduction",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "",
    "text": "In this session, we will focus on fitting various volatility models to financial time series data, specifically the S&P 500 index. We will:\n\nDownload and prepare the S&P 500 data.\nPerform exploratory data analysis (EDA).\nFit different GARCH models.\nEvaluate model diagnostics.\nForecast future volatility.\nCalculate Value at Risk (VaR) and Expected Shortfall (ES)."
  },
  {
    "objectID": "day2_part2_fitting_models.html#loading-required-libraries",
    "href": "day2_part2_fitting_models.html#loading-required-libraries",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\nFirst, we load the necessary R packages for data manipulation, time series analysis, and modeling.\n\nlibrary(tidyverse)     # Data manipulation and visualization\nlibrary(tidyfinance)   # Financial data tools\nlibrary(tsibble)       # Time series data manipulation\nlibrary(fable)         # Time series modeling\nlibrary(feasts)        # Time series features\nlibrary(rugarch)       # Univariate GARCH models\nlibrary(forecast)      # Forecasting functions"
  },
  {
    "objectID": "day2_part2_fitting_models.html#data-acquisition-and-preparation",
    "href": "day2_part2_fitting_models.html#data-acquisition-and-preparation",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\nWe will work with the S&P 500 index data. If the data file does not exist locally, we download it using the download_data function; otherwise, we read it from the existing CSV file.\n\nif (!file.exists(\"sp500_index.csv\")) {\n  sp500_download &lt;-\n    download_data(\n      \"stock_prices\",\n      symbols = \"^GSPC\",\n      start_date = \"2014-11-18\",\n      end_date = \"2024-11-18\"\n    )\n  sp500_download %&gt;% write_csv(\"sp500_index.csv\")\n} else {\n  sp500_download &lt;- read_csv(\"sp500_index.csv\")\n}\n\nRows: 2516 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): symbol\ndbl  (6): volume, open, low, high, close, adjusted_close\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nCalculating Returns\nWe calculate the daily returns of the S&P 500 index from the adjusted closing prices.\n\nsp500_returns &lt;-\n  sp500_download %&gt;%\n  select(symbol, date, price = adjusted_close) %&gt;%\n  mutate(return = price / lag(price) - 1) %&gt;%\n  remove_missing() %&gt;%\n  select(-price) %&gt;%\n  mutate(date = row_number()) %&gt;%\n  print()\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange.\n\n\n# A tibble: 2,515 × 3\n   symbol  date   return\n   &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1 ^GSPC      1 -0.00150\n 2 ^GSPC      2  0.00197\n 3 ^GSPC      3  0.00524\n 4 ^GSPC      4  0.00286\n 5 ^GSPC      5 -0.00115\n 6 ^GSPC      6  0.00281\n 7 ^GSPC      7 -0.00254\n 8 ^GSPC      8 -0.00683\n 9 ^GSPC      9  0.00638\n10 ^GSPC     10  0.00376\n# ℹ 2,505 more rows"
  },
  {
    "objectID": "day2_part2_fitting_models.html#exploratory-data-analysis",
    "href": "day2_part2_fitting_models.html#exploratory-data-analysis",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nPlotting Returns\nWe visualize the daily returns over time to observe any patterns or anomalies.\n\nsp500_returns %&gt;%\n  ggplot(aes(x = date, y = return)) +\n  geom_line() +\n  labs(\n    title = \"S&P 500 Daily Returns\",\n    x = \"Day\",\n    y = \"Daily Return\"\n  )\n\n\n\n\n\n\n\n\n\n\nStatistical Summary\nWe compute key statistical measures to understand the distribution of returns.\n\nsp500_stats &lt;-\n  sp500_returns %&gt;%\n  summarise(\n    mean_return = mean(return),\n    std_dev = sd(return),\n    skewness = moments::skewness(return),\n    kurtosis = moments::kurtosis(return),\n    jarque_bera_pvalue = tseries::jarque.bera.test(return)$p.value\n  )\n\nsp500_stats\n\n# A tibble: 1 × 5\n  mean_return std_dev skewness kurtosis jarque_bera_pvalue\n        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1    0.000481  0.0112   -0.517     17.5                  0\n\n\n\n\nReturn Distribution\nWe compare the empirical distribution of returns to the normal distribution.\n\nsp500_returns %&gt;%\n  ggplot(aes(x = return)) +\n  geom_histogram(aes(y = ..density..), bins = 50, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(sp500_returns$return),\n      sd = sd(sp500_returns$return)\n    ),\n    color = \"blue\",\n    size = 1\n  ) +\n  labs(\n    title = \"Return Distribution vs Normal Distribution\",\n    x = \"Daily Return\",\n    y = \"Density\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nTime Series Characteristics\nWe examine the autocorrelation and partial autocorrelation of the returns.\n\nsp500_returns %&gt;%\n  as_tsibble(index = date) %&gt;%\n  gg_tsdisplay(return, plot_type = \"partial\")"
  },
  {
    "objectID": "day2_part2_fitting_models.html#modeling-volatility",
    "href": "day2_part2_fitting_models.html#modeling-volatility",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Modeling Volatility",
    "text": "Modeling Volatility\n\nFitting an ARIMA Model\nWe fit an ARIMA model to the returns to capture any autoregressive patterns.\n\nfit_auto_arima &lt;-\n  sp500_returns %&gt;%\n  as_tsibble(index = date) %&gt;%\n  model(arima = ARIMA(return))\nfit_auto_arima\n\n# A mable: 1 x 1\n                   arima\n                 &lt;model&gt;\n1 &lt;ARIMA(2,0,2) w/ mean&gt;\n\n\n\n\nSpecifying GARCH and ARMA Orders\nWe define the orders for the GARCH and ARMA components.\n\ngarch_order &lt;- c(2, 2)  # GARCH(p,q) order\narma_order &lt;- c(2, 2)   # ARMA(p,q) order\n\n\n\nDefining GARCH Model Specifications\nWe specify multiple GARCH models to compare their performance.\n\ngarch_specs &lt;-\n  list(\n    # Standard GARCH model specification\n    sGARCH = ugarchspec(\n      variance.model = list(\n        model = \"sGARCH\",\n        garchOrder = garch_order\n      ),\n      mean.model = list(\n        armaOrder = arma_order,\n        include.mean = TRUE\n      ),\n      distribution.model = \"std\"  # Student's t-distribution\n    ),\n    # Asymmetric Power ARCH model specification\n    apARCH = ugarchspec(\n      variance.model = list(\n        model = \"apARCH\",\n        garchOrder = garch_order\n      ),\n      mean.model = list(\n        armaOrder = arma_order,\n        include.mean = TRUE\n      ),\n      distribution.model = \"std\"\n    ),\n    # GJR-GARCH model specification\n    gjrGARCH = ugarchspec(\n      variance.model = list(\n        model = \"gjrGARCH\",\n        garchOrder = garch_order\n      ),\n      mean.model = list(\n        armaOrder = arma_order,\n        include.mean = TRUE\n      ),\n      distribution.model = \"std\"\n    ),\n    # Threshold GARCH (TGARCH) model specification\n    tGARCH = ugarchspec(\n      variance.model = list(\n        model = \"fGARCH\",\n        submodel = \"TGARCH\",\n        garchOrder = garch_order\n      ),\n      mean.model = list(\n        armaOrder = arma_order,\n        include.mean = TRUE\n      ),\n      distribution.model = \"std\"\n    )\n  )\n\n\n\nFitting the GARCH Models\nWe fit each GARCH model to the returns data.\n\nfit_results &lt;-\n  map(\n    garch_specs,\n    ~ ugarchfit(spec = .x, data = sp500_returns$return)\n  )"
  },
  {
    "objectID": "day2_part2_fitting_models.html#model-diagnostics",
    "href": "day2_part2_fitting_models.html#model-diagnostics",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nDefining a Diagnostic Function\nWe create a function to extract important diagnostic statistics from each model.\n\nget_model_diagnostics &lt;- function(model) {\n  tibble(\n    aic = infocriteria(model)[1],\n    bic = infocriteria(model)[2],\n    log_likelihood = likelihood(model),\n    persistence = persistence(model),\n    half_life = -log(2) / log(persistence(model)),\n    unconditional_variance = mean(sigma(model)^2)\n  )\n}\n\n\n\nComparing Model Statistics\nWe apply the diagnostic function to each model and compile the results.\n\nfit_stats &lt;- map_dfr(fit_results, get_model_diagnostics, .id = \"model\")\nfit_stats\n\n# A tibble: 4 × 7\n  model    aic   bic log_likelihood persistence half_life unconditional_variance\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;                  &lt;dbl&gt;\n1 sGARCH -6.70 -6.67          8436.       0.997     201.                0.000136\n2 apARCH -6.74 -6.71          8495.       0.964      18.9               0.000132\n3 gjrGA… -6.73 -6.70          8477.       0.987      53.4               0.000141\n4 tGARCH -6.75 -6.72          8500.       0.942      11.5               0.000135\n\n\n\n\nAnalyzing Residuals\nWe extract standardized residuals to check for remaining autocorrelation or patterns.\n\nresiduals &lt;-\n  imap_dfr(\n    fit_results,\n    ~ tibble(\n      model = .y,\n      date = sp500_returns$date,\n      residuals = residuals(.x, standardize = TRUE) %&gt;% as.numeric()\n    )\n  )\nresiduals\n\n# A tibble: 10,060 × 3\n   model   date residuals\n   &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;\n 1 sGARCH     1   -0.215 \n 2 sGARCH     2    0.0943\n 3 sGARCH     3    0.455 \n 4 sGARCH     4    0.206 \n 5 sGARCH     5   -0.250 \n 6 sGARCH     6    0.233 \n 7 sGARCH     7   -0.481 \n 8 sGARCH     8   -1.08  \n 9 sGARCH     9    0.773 \n10 sGARCH    10    0.396 \n# ℹ 10,050 more rows\n\n\n\n\nPlotting Squared Residuals\nWe plot the squared standardized residuals to assess the adequacy of the volatility models.\n\nresiduals %&gt;%\n  ggplot(aes(x = date, y = residuals^2)) +\n  geom_line(color = \"darkgreen\") +\n  labs(\n    title = \"Squared Standardized Residuals\",\n    x = \"Date\",\n    y = \"Residuals Squared\"\n  ) +\n  facet_wrap(~model)\n\n\n\n\n\n\n\n\n\n\nResidual Diagnostics for tGARCH Model\nWe perform time series diagnostics on the residuals of the tGARCH model.\n\nresiduals %&gt;%\n  filter(model == \"tGARCH\") %&gt;%\n  as_tsibble(index = date) %&gt;%\n  gg_tsdisplay(residuals, plot_type = \"partial\")"
  },
  {
    "objectID": "day2_part2_fitting_models.html#forecasting-volatility",
    "href": "day2_part2_fitting_models.html#forecasting-volatility",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Forecasting Volatility",
    "text": "Forecasting Volatility\n\nGenerating Volatility Forecasts\nWe forecast the future volatility over the next 30 days using each model.\n\nn_ahead &lt;- 30  # Number of days to forecast\nforecasts &lt;- map(\n  fit_results,\n  ~ ugarchforecast(.x, n.ahead = n_ahead)\n)\n\n\n\nPreparing Forecast Data\nWe extract the forecasted volatility and compute confidence intervals.\n\nforecast_data &lt;-\n  map2_df(\n    forecasts, names(forecasts),\n    ~ {\n      sigma_forecast &lt;- sigma(.x) %&gt;% as.numeric()\n      conf_intervals &lt;- qnorm(c(0.025, 0.975)) * sigma_forecast\n      tibble(\n        Day = 1:n_ahead,\n        Forecasted_Volatility = sigma_forecast,\n        Lower_CI = sigma_forecast + conf_intervals[1],\n        Upper_CI = sigma_forecast + conf_intervals[2],\n        Model = .y\n      )\n    }\n  )\n\n\n\nPlotting Forecasted Volatility\nWe visualize the forecasted volatility for each model.\n\nggplot(forecast_data, aes(x = Day, y = Forecasted_Volatility, color = Model)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"30-Day Ahead Volatility Forecasts\",\n    x = \"Day\",\n    y = \"Forecasted Volatility (Sigma)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "day2_part2_fitting_models.html#risk-assessment-var-and-es",
    "href": "day2_part2_fitting_models.html#risk-assessment-var-and-es",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Risk Assessment: VaR and ES",
    "text": "Risk Assessment: VaR and ES\n\nCalculating VaR and ES\nWe compute the Value at Risk (VaR) and Expected Shortfall (ES) at the 95% confidence level for each model.\n\nalpha &lt;- 0.05  # 95% confidence level\n\n# Iterate over forecasts and fitted models\nvar_forecasts &lt;- map2_dfr(forecasts, fit_results, function(forecast, model) {\n  # Extract mean and standard deviation from the forecast\n  mu &lt;- as.numeric(fitted(forecast))[1]\n  sigma &lt;- as.numeric(sigma(forecast))[1]\n\n  # Extract the shape parameter (degrees of freedom)\n  shape &lt;- as.numeric(coef(model)[\"shape\"])\n\n  # Calculate VaR using the t-distribution quantile\n  var_95 &lt;- mu + sigma * qt(alpha, df = shape)\n\n  # Calculate ES (Expected Shortfall)\n  qt_value &lt;- qt(alpha, df = shape)\n  dt_value &lt;- dt(qt_value, df = shape)\n  es_95 &lt;- mu - sigma * (dt_value / alpha) * (shape + qt_value^2) / (shape - 1)\n\n  tibble(\n    Model = model@model$modeldesc$vmodel,\n    VaR_95 = var_95,\n    ES_95 = es_95\n  )\n}, .id = \"model\")\n\n\n\nDisplaying Risk Measures\nWe present the calculated VaR and ES for each volatility model.\n\nvar_forecasts\n\n# A tibble: 4 × 4\n  model    Model     VaR_95   ES_95\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 sGARCH   sGARCH   -0.0192 -0.0272\n2 apARCH   apARCH   -0.0194 -0.0269\n3 gjrGARCH gjrGARCH -0.0201 -0.0282\n4 tGARCH   fGARCH   -0.0209 -0.0291"
  },
  {
    "objectID": "day2_part2_fitting_models.html#conclusion",
    "href": "day2_part2_fitting_models.html#conclusion",
    "title": "Day 2 Part 1: Fitting Volatility Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we:\n\nCollected and prepared S&P 500 index data.\nPerformed exploratory data analysis to understand the return characteristics.\nSpecified and fitted various GARCH models to capture volatility clustering and leverage effects.\nEvaluated model diagnostics to select the most appropriate model.\nForecasted future volatility over a 30-day horizon.\nCalculated risk measures such as Value at Risk (VaR) and Expected Shortfall (ES) to assess potential losses.\n\nThis comprehensive approach demonstrates how volatility modeling is crucial in financial econometrics for risk management and forecasting."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zero to Hero Bootcamp",
    "section": "",
    "text": "Day 2 of this workshop builds upon the foundational concepts covered in Day 1, delving deeper into advanced topics in time series econometrics using R. The focus will be on modeling volatility processes, fitting GARCH models to financial data, and applying these techniques to real-world financial datasets.\nIndicative Topics:\n\nSimulating Volatility Processes\n\nUnderstanding different types of GARCH models.\nSimulating time series data using specified GARCH models.\nVisualizing and analyzing simulated volatility processes.\n\nFitting Volatility Models\n\nDownloading and preparing financial time series data.\nPerforming exploratory data analysis (EDA).\nFitting various GARCH models to real-world data.\nEvaluating model diagnostics and selecting appropriate models.\nForecasting future volatility and calculating risk measures like VaR and ES.\n\nApplication to S&P 500 Stock Portfolio\n\nConstructing a minimum variance portfolio.\nApplying GARCH models to portfolio returns.\nPerforming rolling forecasts and backtesting.\nAssessing portfolio risk using advanced econometric techniques.\n\n\n\n\n\nBy the end of the second workshop day, participants will be able to:\n\nSimulate and analyze various GARCH models to understand volatility dynamics.\nFit GARCH models to financial time series data and interpret the results.\nUse model diagnostics to evaluate and select appropriate volatility models.\nForecast future volatility and calculate risk measures such as Value at Risk (VaR) and Expected Shortfall (ES).\nApply volatility modeling techniques to construct and analyze stock portfolios.\n\n\n\n\n\nDate: 23 November 2024\nTime: 10:00-16:00\nLocation: WB-0001 (Google Maps Location)\n\n\n\n\n\nYour own laptop\nInstallation of R and RStudio\n\nFor R, follow this link\nFor RStudio, follow this link\nOn Windows, also install RTools 4.4 from this link\n\nInstallation of R packages (details below)\n\nPlease prepare — all download and installation steps can take 1 hour, or longer! If you already have an installation, please update to the newest version.\nTo install the necessary packages, run this code in RStudio:\nposit_repo &lt;- \n  \"https://packagemanager.posit.co/cran/latest\"\n\nworkshop_packages &lt;- \n  c(\n    \"tidyverse\",\n    \"tsibble\",\n    \"feasts\",\n    \"fable\",\n    \"tsgarch\",\n    \"tidyfinance\",\n    \"httr2\",\n    \"fixest\",\n    \"flextable\",\n    \"xts\",\n    \"urca\",\n    \"rugarch\",\n    \"PortfolioAnalytics\",\n    \"ggpubr\",\n    \"forecast\",\n    \"tseries\",\n    \"broom\",\n    \"moments\"\n  )\n\ninstall.packages(\n    workshop_packages, \n    repos = posit_repo\n  )\n\n\n\n\n\nIn this session, we will:\n\nSpecify Different GARCH Models: Learn how to define various GARCH models such as sGARCH, GJR-GARCH, eGARCH, and apARCH using the rugarch package.\nSimulate Time Series Data: Use the specified models to simulate time series data and understand the behavior of different volatility processes.\nVisualize and Analyze Simulated Data: Plot the simulated series and their corresponding volatilities to observe the characteristics and differences among the models.\n\nKey Activities:\n\nSpecifying GARCH models using ugarchspec.\nSimulating data with ugarchpath.\nVisualizing time series and volatility processes using ggplot2 and tsibble.\n\n\n\n\nIn this session, we will:\n\nDownload and Prepare Financial Data: Acquire S&P 500 index data and calculate daily returns.\nPerform Exploratory Data Analysis (EDA): Analyze the statistical properties of the returns, including mean, standard deviation, skewness, and kurtosis.\nFit GARCH Models to Real Data: Apply different GARCH models to the returns data using ugarchfit.\nEvaluate Model Diagnostics: Use criteria such as AIC and BIC to select the best-fitting model.\nForecast Future Volatility and Risk Measures: Generate volatility forecasts and calculate Value at Risk (VaR) and Expected Shortfall (ES).\n\nKey Activities:\n\nData manipulation and visualization with tidyverse.\nModel fitting and diagnostics using rugarch.\nRisk assessment calculations for VaR and ES.\n\n\n\n\nIn this session, we will:\n\nConstruct a Minimum Variance Portfolio: Optimize a portfolio of S&P 500 stocks to minimize risk using the PortfolioAnalytics package.\nCalculate Portfolio Returns: Compute the daily returns of the optimized portfolio.\nFit a GARCH Model to Portfolio Returns: Model the volatility of the portfolio returns with a TGARCH model.\nPerform Rolling Forecasts and Backtesting: Use ugarchroll to evaluate the model’s performance over time and assess its predictive power.\nAssess Portfolio Risk: Analyze the Value at Risk (VaR) and backtest the model to ensure its adequacy.\n\nKey Activities:\n\nPortfolio optimization and risk management.\nTime series modeling of portfolio returns.\nRolling forecasts and backtesting with rugarch.\nVisualization and interpretation of results."
  },
  {
    "objectID": "index.html#day-2-advanced-time-series-econometrics-in-r",
    "href": "index.html#day-2-advanced-time-series-econometrics-in-r",
    "title": "Zero to Hero Bootcamp",
    "section": "",
    "text": "Day 2 of this workshop builds upon the foundational concepts covered in Day 1, delving deeper into advanced topics in time series econometrics using R. The focus will be on modeling volatility processes, fitting GARCH models to financial data, and applying these techniques to real-world financial datasets.\nIndicative Topics:\n\nSimulating Volatility Processes\n\nUnderstanding different types of GARCH models.\nSimulating time series data using specified GARCH models.\nVisualizing and analyzing simulated volatility processes.\n\nFitting Volatility Models\n\nDownloading and preparing financial time series data.\nPerforming exploratory data analysis (EDA).\nFitting various GARCH models to real-world data.\nEvaluating model diagnostics and selecting appropriate models.\nForecasting future volatility and calculating risk measures like VaR and ES.\n\nApplication to S&P 500 Stock Portfolio\n\nConstructing a minimum variance portfolio.\nApplying GARCH models to portfolio returns.\nPerforming rolling forecasts and backtesting.\nAssessing portfolio risk using advanced econometric techniques.\n\n\n\n\n\nBy the end of the second workshop day, participants will be able to:\n\nSimulate and analyze various GARCH models to understand volatility dynamics.\nFit GARCH models to financial time series data and interpret the results.\nUse model diagnostics to evaluate and select appropriate volatility models.\nForecast future volatility and calculate risk measures such as Value at Risk (VaR) and Expected Shortfall (ES).\nApply volatility modeling techniques to construct and analyze stock portfolios.\n\n\n\n\n\nDate: 23 November 2024\nTime: 10:00-16:00\nLocation: WB-0001 (Google Maps Location)\n\n\n\n\n\nYour own laptop\nInstallation of R and RStudio\n\nFor R, follow this link\nFor RStudio, follow this link\nOn Windows, also install RTools 4.4 from this link\n\nInstallation of R packages (details below)\n\nPlease prepare — all download and installation steps can take 1 hour, or longer! If you already have an installation, please update to the newest version.\nTo install the necessary packages, run this code in RStudio:\nposit_repo &lt;- \n  \"https://packagemanager.posit.co/cran/latest\"\n\nworkshop_packages &lt;- \n  c(\n    \"tidyverse\",\n    \"tsibble\",\n    \"feasts\",\n    \"fable\",\n    \"tsgarch\",\n    \"tidyfinance\",\n    \"httr2\",\n    \"fixest\",\n    \"flextable\",\n    \"xts\",\n    \"urca\",\n    \"rugarch\",\n    \"PortfolioAnalytics\",\n    \"ggpubr\",\n    \"forecast\",\n    \"tseries\",\n    \"broom\",\n    \"moments\"\n  )\n\ninstall.packages(\n    workshop_packages, \n    repos = posit_repo\n  )\n\n\n\n\n\nIn this session, we will:\n\nSpecify Different GARCH Models: Learn how to define various GARCH models such as sGARCH, GJR-GARCH, eGARCH, and apARCH using the rugarch package.\nSimulate Time Series Data: Use the specified models to simulate time series data and understand the behavior of different volatility processes.\nVisualize and Analyze Simulated Data: Plot the simulated series and their corresponding volatilities to observe the characteristics and differences among the models.\n\nKey Activities:\n\nSpecifying GARCH models using ugarchspec.\nSimulating data with ugarchpath.\nVisualizing time series and volatility processes using ggplot2 and tsibble.\n\n\n\n\nIn this session, we will:\n\nDownload and Prepare Financial Data: Acquire S&P 500 index data and calculate daily returns.\nPerform Exploratory Data Analysis (EDA): Analyze the statistical properties of the returns, including mean, standard deviation, skewness, and kurtosis.\nFit GARCH Models to Real Data: Apply different GARCH models to the returns data using ugarchfit.\nEvaluate Model Diagnostics: Use criteria such as AIC and BIC to select the best-fitting model.\nForecast Future Volatility and Risk Measures: Generate volatility forecasts and calculate Value at Risk (VaR) and Expected Shortfall (ES).\n\nKey Activities:\n\nData manipulation and visualization with tidyverse.\nModel fitting and diagnostics using rugarch.\nRisk assessment calculations for VaR and ES.\n\n\n\n\nIn this session, we will:\n\nConstruct a Minimum Variance Portfolio: Optimize a portfolio of S&P 500 stocks to minimize risk using the PortfolioAnalytics package.\nCalculate Portfolio Returns: Compute the daily returns of the optimized portfolio.\nFit a GARCH Model to Portfolio Returns: Model the volatility of the portfolio returns with a TGARCH model.\nPerform Rolling Forecasts and Backtesting: Use ugarchroll to evaluate the model’s performance over time and assess its predictive power.\nAssess Portfolio Risk: Analyze the Value at Risk (VaR) and backtest the model to ensure its adequacy.\n\nKey Activities:\n\nPortfolio optimization and risk management.\nTime series modeling of portfolio returns.\nRolling forecasts and backtesting with rugarch.\nVisualization and interpretation of results."
  },
  {
    "objectID": "index.html#day-1-fundamentals-of-time-series-econometrics-in-r",
    "href": "index.html#day-1-fundamentals-of-time-series-econometrics-in-r",
    "title": "Zero to Hero Bootcamp",
    "section": "Day 1: Fundamentals of Time Series Econometrics in R",
    "text": "Day 1: Fundamentals of Time Series Econometrics in R\n\nAims\nDay 1 of this workshop introduces participants to foundational concepts and techniques in time series analysis using R. It focuses on equipping attendees with practical skills for handling, analyzing, and modeling time series data, with applications relevant to finance and economics.\nIndicative Topics:\n\nAnalysis with Tidyverse\nExploring Time Series Data\nStationarity and Random Walks\nARIMA Modeling\nPreview of Volatility Processes\n\n\n\nLearning Outcomes\nBy the end of the first workshop day, participants will be able to:\n\nUse Tidyverse tools for data analysis in R.\nExplore and visualize time series data effectively.\nUnderstand and apply concepts of random walks and stationarity.\nDevelop and interpret ARIMA models for forecasting.\nGain insight into volatility processes.\n\n\n\nWhere and When?\n\nDate: 16 November\nTime: 10:00-16:00\nLocation: WB-0001 (Google Maps Location)\n\n\n\nWhat to Bring?\n\nYour own laptop\nInstallation of R and RStudio\n\nFor R, follow this link\nFor RStudio, follow this link\nOn Windows, also install RTools 4.4 from this link\n\nInstallation of R packages (details below)\n\nPlease prepare — all download and installation steps can take 1 hour, or longer! If you already have an installation, please update to the newest version.\nAfter installing R, run this command:\nR.Version()\nAnd verify that your R version is the following:\n\"R version 4.4.2 (2024-10-31)\"\n\"Pile of Leaves\"\nIn RStudio, verify that this is your version:\nVersion 2024.09.1+394 (2024.09.1+394)\nTo install the necessary packages, run this code in RStudio:\nposit_repo &lt;- \n  \"https://packagemanager.posit.co/cran/latest\"\n\ninstall.packages(\n    \"renv\", \n    repos = posit_repo,\n    ask = FALSE,\n    dependencies = TRUE\n  )\n\nworkshop_packages &lt;- \n    c(\n      \"tidyverse\",\n      \"tsibble\",\n      \"feasts\",\n      \"fable\",\n      \"tsgarch\",\n      \"tidyfinance\",\n      \"httr2\",\n      \"fixest\",\n      \"flextable\",\n      \"xts\",\n      \"urca\"\n    )\n\nrenv::install(\n    workshop_packages, \n    repos = posit_repo, \n    dependencies = \"all\",\n    prompt = FALSE\n  )\n\n\nUseful Resources\n\nTextbook: “Applied Econometric Time Series (3e)” by Walter Enders\nOnline Book: Data Science for R\n\nChapter “Introduction”\nChapter “Whole Game”\n\nOnline Book: Tidy Finance with R\n\nChapter “Accessing and Managing Financial Data”\n\nOnline Book: Forecasting: Principles and Practice\n\nChapter 7: Time Series Regression Models\nChapter 9: ARIMA Models"
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "Zero to Hero Bootcamp",
    "section": "Questions?",
    "text": "Questions?\nTwo options:\n\nPost on this Padlet (password: ts-workshop). This is the preferred option as there will be a lot of participants at the workshop.\nContact me at christian_engels@outlook.com. I may not be able to respond to all requests, but I will try my best!"
  },
  {
    "objectID": "day1_part1_tidyverse.html",
    "href": "day1_part1_tidyverse.html",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries are essential for handling, transforming, and analyzing time series data, providing tools for visualization, statistical analysis, and econometric modeling.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)"
  },
  {
    "objectID": "day1_part1_tidyverse.html#load-libraries",
    "href": "day1_part1_tidyverse.html#load-libraries",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries are essential for handling, transforming, and analyzing time series data, providing tools for visualization, statistical analysis, and econometric modeling.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)"
  },
  {
    "objectID": "day1_part1_tidyverse.html#download-data",
    "href": "day1_part1_tidyverse.html#download-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Download Data",
    "text": "Download Data\nDownload and print data from FRED for selected series: GDP and CPIAUCNS (Consumer Price Index). The Federal Reserve Economic Data (FRED) repository provides a rich source of economic time series data, which we can use to perform exploratory analysis and model economic trends.\n\nfred &lt;- download_data(\"fred\", series = c(\"GDP\", \"CPIAUCNS\"))\n\nNo `start_date` or `end_date` provided. Returning the full data set.\n\nfred\n\n# A tibble: 1,653 × 3\n   date       value series\n   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1 1947-01-01  243. GDP   \n 2 1947-04-01  246. GDP   \n 3 1947-07-01  250. GDP   \n 4 1947-10-01  260. GDP   \n 5 1948-01-01  266. GDP   \n 6 1948-04-01  273. GDP   \n 7 1948-07-01  279. GDP   \n 8 1948-10-01  280. GDP   \n 9 1949-01-01  275. GDP   \n10 1949-04-01  271. GDP   \n# ℹ 1,643 more rows"
  },
  {
    "objectID": "day1_part1_tidyverse.html#print-and-view-data",
    "href": "day1_part1_tidyverse.html#print-and-view-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Print and View Data",
    "text": "Print and View Data\nPrint the data and view it in the viewer for a detailed look. Viewing the data in this manner allows us to understand the structure and content of the dataset, which is helpful for planning further analysis steps.\n\nView(fred)\n\nfred %&gt;% View"
  },
  {
    "objectID": "day1_part1_tidyverse.html#summary-of-data",
    "href": "day1_part1_tidyverse.html#summary-of-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Summary of Data",
    "text": "Summary of Data\nDisplay a summary glimpse of the FRED data. A quick overview of the dataset’s variables helps verify data types, spot missing values, and understand the overall data structure.\n\nfred %&gt;% glimpse()\n\nRows: 1,653\nColumns: 3\n$ date   &lt;date&gt; 1947-01-01, 1947-04-01, 1947-07-01, 1947-10-01, 1948-01-01, 19…\n$ value  &lt;dbl&gt; 243.164, 245.968, 249.585, 259.745, 265.742, 272.567, 279.196, …\n$ series &lt;chr&gt; \"GDP\", \"GDP\", \"GDP\", \"GDP\", \"GDP\", \"GDP\", \"GDP\", \"GDP\", \"GDP\", …"
  },
  {
    "objectID": "day1_part1_tidyverse.html#filter-for-cpiaucns-series",
    "href": "day1_part1_tidyverse.html#filter-for-cpiaucns-series",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Filter for CPIAUCNS Series",
    "text": "Filter for CPIAUCNS Series\nFilter the FRED data to show only the CPIAUCNS series. Filtering for the Consumer Price Index series allows us to focus on inflation data, which is an important economic indicator for understanding price stability.\n\nfred %&gt;% filter(series == \"CPIAUCNS\")\n\n# A tibble: 1,342 × 3\n   date       value series  \n   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1 1913-01-01   9.8 CPIAUCNS\n 2 1913-02-01   9.8 CPIAUCNS\n 3 1913-03-01   9.8 CPIAUCNS\n 4 1913-04-01   9.8 CPIAUCNS\n 5 1913-05-01   9.7 CPIAUCNS\n 6 1913-06-01   9.8 CPIAUCNS\n 7 1913-07-01   9.9 CPIAUCNS\n 8 1913-08-01   9.9 CPIAUCNS\n 9 1913-09-01  10   CPIAUCNS\n10 1913-10-01  10   CPIAUCNS\n# ℹ 1,332 more rows"
  },
  {
    "objectID": "day1_part1_tidyverse.html#summary-statistics-for-cpiaucns",
    "href": "day1_part1_tidyverse.html#summary-statistics-for-cpiaucns",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Summary Statistics for CPIAUCNS",
    "text": "Summary Statistics for CPIAUCNS\nCalculate the earliest date, latest date, and mean value for the CPIAUCNS series. These summary statistics provide insights into the temporal coverage of the data and the average level of consumer prices over the observed period.\n\nfred %&gt;% \n  filter(series == \"CPIAUCNS\") %&gt;% \n  summarise(\n    min(date), \n    max(date), \n    mean(value)\n  )\n\n# A tibble: 1 × 3\n  `min(date)` `max(date)` `mean(value)`\n  &lt;date&gt;      &lt;date&gt;              &lt;dbl&gt;\n1 1913-01-01  2024-10-01           88.9\n\n\nProvide an enhanced summary of CPIAUCNS with labeled output for minimum and maximum dates, and mean value. This labeled summary helps in clearly interpreting the results, which is useful for documentation and reporting.\n\nfred %&gt;% \n  filter(series == \"CPIAUCNS\") %&gt;% \n  summarise(\n    date_min = min(date),\n    date_max = max(date),\n    value_mean = mean(value)\n  )\n\n# A tibble: 1 × 3\n  date_min   date_max   value_mean\n  &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 1913-01-01 2024-10-01       88.9"
  },
  {
    "objectID": "day1_part1_tidyverse.html#summary-for-all-series",
    "href": "day1_part1_tidyverse.html#summary-for-all-series",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Summary for All Series",
    "text": "Summary for All Series\nSummarize minimum and maximum dates, and mean value for each series in the FRED data. Summarizing each series individually allows us to compare the temporal coverage and mean values, providing a comparative snapshot of the key metrics for GDP and CPI.\n\nfred %&gt;% \n  group_by(series) %&gt;% \n  summarise(\n    date_min = min(date),\n    date_max = max(date),\n    value_mean = mean(value)\n  )\n\n# A tibble: 2 × 4\n  series   date_min   date_max   value_mean\n  &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 CPIAUCNS 1913-01-01 2024-10-01       88.9\n2 GDP      1947-01-01 2024-07-01     7380."
  },
  {
    "objectID": "day1_part1_tidyverse.html#yearly-mean-value",
    "href": "day1_part1_tidyverse.html#yearly-mean-value",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Yearly Mean Value",
    "text": "Yearly Mean Value\nCalculate the yearly mean value for each series in the FRED data. Calculating yearly means helps smooth out short-term fluctuations and reveal longer-term trends, which is important for econometric modeling.\n\nfred %&gt;% \n  group_by(series, year = year(date)) %&gt;% \n  summarise(value_mean = mean(value))\n\n`summarise()` has grouped output by 'series'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 190 × 3\n# Groups:   series [2]\n   series    year value_mean\n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 CPIAUCNS  1913       9.88\n 2 CPIAUCNS  1914      10.0 \n 3 CPIAUCNS  1915      10.1 \n 4 CPIAUCNS  1916      10.9 \n 5 CPIAUCNS  1917      12.8 \n 6 CPIAUCNS  1918      15.0 \n 7 CPIAUCNS  1919      17.3 \n 8 CPIAUCNS  1920      20.0 \n 9 CPIAUCNS  1921      17.8 \n10 CPIAUCNS  1922      16.8 \n# ℹ 180 more rows"
  },
  {
    "objectID": "day1_part1_tidyverse.html#pivot-yearly-mean-data",
    "href": "day1_part1_tidyverse.html#pivot-yearly-mean-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Pivot Yearly Mean Data",
    "text": "Pivot Yearly Mean Data\nPivot the yearly mean data to show each series as a column for easier comparison. Pivoting the data allows us to easily compare trends across different economic indicators, making it more efficient to analyze relationships between GDP and CPI.\n\nfred %&gt;% \n  group_by(series, year = year(date)) %&gt;% \n  summarise(value_mean = mean(value)) %&gt;% \n  pivot_wider(\n    id_cols = year,\n    names_from = series,\n    values_from = value_mean\n  )\n\n`summarise()` has grouped output by 'series'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 112 × 3\n    year CPIAUCNS   GDP\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1  1913     9.88    NA\n 2  1914    10.0     NA\n 3  1915    10.1     NA\n 4  1916    10.9     NA\n 5  1917    12.8     NA\n 6  1918    15.0     NA\n 7  1919    17.3     NA\n 8  1920    20.0     NA\n 9  1921    17.8     NA\n10  1922    16.8     NA\n# ℹ 102 more rows"
  },
  {
    "objectID": "day1_part1_tidyverse.html#summarize-and-reshape-yearly-data",
    "href": "day1_part1_tidyverse.html#summarize-and-reshape-yearly-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Summarize and Reshape Yearly Data",
    "text": "Summarize and Reshape Yearly Data\nSummarize and reshape the data to show yearly averages for each series, then pivot to make each series a column and remove any missing data. Removing missing values after reshaping ensures a clean dataset, which is crucial for accurate analysis and avoids issues during statistical modeling.\n\nfred_yearly &lt;- \n  fred %&gt;% \n  group_by(series, year = year(date)) %&gt;% \n  summarise(value_mean = mean(value)) %&gt;% \n  pivot_wider(\n    id_cols = year,\n    names_from = series,\n    values_from = value_mean\n  ) %&gt;% \n  remove_missing()\n\n`summarise()` has grouped output by 'series'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 34 rows containing missing values or values outside the scale\nrange.\n\nfred_yearly\n\n# A tibble: 78 × 3\n    year CPIAUCNS   GDP\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1  1947     22.3  250.\n 2  1948     24.0  274.\n 3  1949     23.8  272.\n 4  1950     24.1  300.\n 5  1951     26.0  347.\n 6  1952     26.6  367.\n 7  1953     26.8  389.\n 8  1954     26.8  391.\n 9  1955     26.8  425.\n10  1956     27.2  449.\n# ℹ 68 more rows"
  },
  {
    "objectID": "day1_part1_tidyverse.html#inspect-gdp-data",
    "href": "day1_part1_tidyverse.html#inspect-gdp-data",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Inspect GDP Data",
    "text": "Inspect GDP Data\nSelect and inspect the GDP data, organizing it by year. Inspecting GDP specifically allows us to focus on this key economic indicator, preparing it for further analysis such as trend examination and forecasting.\n\nGDP &lt;- \n  fred_yearly %&gt;% \n  select(year, GDP) %&gt;% \n  glimpse()\n\nRows: 78\nColumns: 2\n$ year &lt;dbl&gt; 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957,…\n$ GDP  &lt;dbl&gt; 249.6155, 274.4678, 272.4753, 299.8272, 346.9133, 367.3408, 389.2…"
  },
  {
    "objectID": "day1_part1_tidyverse.html#plot-gdp-over-time",
    "href": "day1_part1_tidyverse.html#plot-gdp-over-time",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Plot GDP Over Time",
    "text": "Plot GDP Over Time\nConvert GDP data to a time-series format and plot it over time. Visualizing GDP over time allows us to identify potential trends, cycles, or structural breaks, which is an important preliminary step before formal econometric modeling.\n\nGDP %&gt;% \n  as_tsibble(index=year) %&gt;% \n  autoplot(.vars=GDP)"
  },
  {
    "objectID": "day1_part1_tidyverse.html#logarithmic-gdp-analysis",
    "href": "day1_part1_tidyverse.html#logarithmic-gdp-analysis",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Logarithmic GDP Analysis",
    "text": "Logarithmic GDP Analysis\nConvert GDP data to a time-series format, add a column with the log of GDP, and plot the logarithmic GDP over time to analyze growth trends. Taking the logarithm of GDP helps linearize exponential growth patterns, making it easier to interpret percentage changes and apply econometric models that assume linear relationships.\n\nGDP %&gt;% \n  as_tsibble(index=year) %&gt;% \n  mutate(log_GDP = log(GDP)) %&gt;% \n  autoplot(.vars = log_GDP)"
  },
  {
    "objectID": "day1_part1_tidyverse.html#exercises",
    "href": "day1_part1_tidyverse.html#exercises",
    "title": "Day 1 Part 1: Analysis with Tidyverse",
    "section": "Exercises",
    "text": "Exercises\nIn this exercise, you will download data from FRED and inspect it.\n\nVisit FRED Economic Data St. Louis FED\nIdentify a time series that interests you and download it using the data_download function (e.g., MSPUS here)\nPlot the time series and aggregate it to the yearly frequency using at least three of the useful summarise functions in dplyr\n\nThroughout, use the print, glimpse and View functions to keep track of your data."
  },
  {
    "objectID": "day1_part4_arima.html",
    "href": "day1_part4_arima.html",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries will help us handle time series data, conduct ARIMA modeling, and visualize results effectively.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)\nlibrary(fabletools)"
  },
  {
    "objectID": "day1_part4_arima.html#load-libraries",
    "href": "day1_part4_arima.html#load-libraries",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "",
    "text": "Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries will help us handle time series data, conduct ARIMA modeling, and visualize results effectively.\n\nlibrary(tidyverse)\nlibrary(tidyfinance)\nlibrary(tsibble)\nlibrary(fable)\nlibrary(feasts)\nlibrary(scales)\nlibrary(fabletools)"
  },
  {
    "objectID": "day1_part4_arima.html#arima-processes",
    "href": "day1_part4_arima.html#arima-processes",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "ARIMA Processes",
    "text": "ARIMA Processes\nARIMA (AutoRegressive Integrated Moving Average) processes are commonly used in time series analysis to model and predict future values by combining autoregressive (AR), moving average (MA), and differencing components. The AR part captures the influence of past values, the MA part captures the influence of past forecast errors, and differencing ensures the series is stationary. ARIMA models are particularly useful for understanding trends and cyclic behaviors in time series data.\nAn autoregressive model of order p, denoted as AR(p), is written as:\n\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\varepsilon_t,\n\nwhere \\varepsilon_t is white noise. This structure resembles a multiple regression with lagged values of y_t as predictors.\nIn contrast, a moving average model of order q, denoted MA(q), uses past forecast errors as predictors:\n\ny_t = c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\cdots + \\theta_q \\varepsilon_{t-q},\n\nwhere \\varepsilon_t is white noise. Since we do not directly observe the values of \\varepsilon_t, it’s not a true regression in the usual sense.\nWhen differencing is combined with AR and MA components, we get a non-seasonal ARIMA model, where “integration” refers to the differencing process. The full model is written as:\n\ny'_t = c + \\phi_1 y'_{t-1} + \\cdots + \\phi_p y'_{t-p} + \\theta_1 \\varepsilon_{t-1} + \\cdots + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t,\n\nwhere y'_t is the differenced series. This is an ARIMA(p, d, q) model, where:\n\np is the order of the autoregressive part,\nd is the degree of differencing required,\nq is the order of the moving average part.\n\nThe ARIMA model framework is powerful for capturing both trend and cyclical behaviors in time series data, making it a fundamental tool in forecasting."
  },
  {
    "objectID": "day1_part4_arima.html#simulate-arima-processes",
    "href": "day1_part4_arima.html#simulate-arima-processes",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "Simulate ARIMA Processes",
    "text": "Simulate ARIMA Processes\nHere we simulate ARIMA processes to illustrate their characteristics.\n\nset.seed(42)\n\nn &lt;- 10^2\ninnovations &lt;- \n  tibble(\n    time = 1:n, \n    e = rnorm(n),\n    e_lag = lag(e, default = 0),\n  ) %&gt;% \n  as_tsibble(index = time)\n\ninnovations\n\n# A tsibble: 100 x 3 [1]\n    time       e   e_lag\n   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1  1.37    0     \n 2     2 -0.565   1.37  \n 3     3  0.363  -0.565 \n 4     4  0.633   0.363 \n 5     5  0.404   0.633 \n 6     6 -0.106   0.404 \n 7     7  1.51   -0.106 \n 8     8 -0.0947  1.51  \n 9     9  2.02   -0.0947\n10    10 -0.0627  2.02  \n# ℹ 90 more rows\n\n\n\nPlot Innovations\nVisualize the random innovations generated for the ARIMA processes. This helps us understand the underlying noise component that drives the stochastic processes in ARIMA models.\n\ninnovations %&gt;% autoplot(e)\n\n\n\n\n\n\n\ninnovations %&gt;% \n  ggplot(aes(e)) + \n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nGenerate ARIMA Processes\nGenerate different ARIMA processes using the simulated innovations. We create ARMA(1,0), ARMA(0,1), and ARMA(1,1) processes to illustrate how varying the parameters affects the behavior of the time series.\n\narma_processes &lt;- \n  innovations %&gt;% \n  mutate(\n    random_walk = accumulate(\n      e, \n      (\\(y_lag, e) y_lag + e)\n    ),\n    arma10 = accumulate(\n      e, \n      (\\(y_lag, e) 0.9*y_lag + e)\n    ),\n    arma01 = -0.9*e_lag + e,\n    arma11 = accumulate2(\n      e, \n      e_lag,\n      .f = (\\(y_lag, e, e_lag) 0.9 * y_lag + 0.9 * e_lag + e),\n      .init = 0\n    )[-1]  \n  )\n\narma_processes\n\n# A tsibble: 100 x 7 [1]\n    time       e   e_lag random_walk arma10 arma01 arma11\n   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1  1.37    0            1.37   1.37   1.37    1.37\n 2     2 -0.565   1.37         0.806  0.669 -1.80    1.90\n 3     3  0.363  -0.565        1.17   0.965  0.871   1.57\n 4     4  0.633   0.363        1.80   1.50   0.306   2.37\n 5     5  0.404   0.633        2.21   1.76  -0.165   3.11\n 6     6 -0.106   0.404        2.10   1.47  -0.470   3.05\n 7     7  1.51   -0.106        3.61   2.84   1.61    4.16\n 8     8 -0.0947  1.51         3.52   2.46  -1.46    5.01\n 9     9  2.02   -0.0947       5.54   4.23   2.10    6.45\n10    10 -0.0627  2.02         5.47   3.75  -1.88    7.56\n# ℹ 90 more rows\n\n\n\n\nDisplay ARIMA Components\nVisualize the generated ARIMA components using their autocorrelation functions (ACF) and partial autocorrelation functions (PACF). This helps us understand the persistence, memory, and dependence structures within each process.\nIn time series analysis, autocorrelation quantifies the relationship between values of a series at different time lags, offering insights into how past values influence future values within the same series. Unlike simple correlation, which measures the linear relationship between two distinct variables, autocorrelation assesses dependencies within a time series by examining lagged values.\nEach lag in a time series has an associated autocorrelation coefficient, with r_1 representing the correlation between y_t and y_{t-1}, r_2 capturing the relationship between y_t and y_{t-2}, and so forth. The formula to calculate the autocorrelation at lag k is:\n\nr_k = \\frac{\\sum_{t=k+1}^T (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^T (y_t - \\bar{y})^2},\n\nwhere T denotes the length of the time series, and \\bar{y} is the mean of the series. The set of autocorrelation coefficients for various lags forms the autocorrelation function (ACF), which helps identify patterns and dependencies in the data.\nHowever, an ACF plot can sometimes be misleading. For example, if y_t and y_{t-1} are correlated, then y_t and y_{t-2} may also appear correlated—not due to new information in y_{t-2}, but simply because both are related to y_{t-1}. This can create a chain of dependencies that may not accurately reflect the unique influence of each lag.\nTo address this issue, we use partial autocorrelations, which measure the relationship between y_t and y_{t-k} after controlling for the influence of intermediate lags (1 through k-1). The first partial autocorrelation is the same as the first autocorrelation, as there are no intervening lags to account for. Each subsequent partial autocorrelation isolates the direct effect of that specific lag, removing the influence of shorter lags. In an autoregressive model, each partial autocorrelation can be estimated as the final coefficient in an AR model of that order. Specifically, the kth partial autocorrelation, \\alpha_k, corresponds to the coefficient \\phi_k in an AR(k) model.\nThis refined approach gives us the partial autocorrelation function (PACF), which provides a clearer picture of the direct relationships between values at specific lags, making it a valuable tool for model selection and interpretation in time series analysis.\n\narma_processes %&gt;% \n  gg_tsdisplay(\n    arma10, \n    plot_type = \"partial\", \n    lag_max = 12\n    )\n\n\n\n\n\n\n\narma_processes %&gt;%\n  gg_tsdisplay(\n    arma01, \n    plot_type = \"partial\", \n    lag_max = 12\n    )\n\n\n\n\n\n\n\narma_processes %&gt;% \n  gg_tsdisplay(\n    arma11, \n    plot_type = \"partial\", \n    lag_max = 12\n    )\n\n\n\n\n\n\n\n\nFor data following an ARIMA model with either AR terms (ARIMA(p, d, 0)) or MA terms (ARIMA(0, d, q)), the ACF and PACF plots can assist in identifying the appropriate values of p or q. When both parameters are positive, however, the plots may not distinctly guide the selection of these values.\nTo identify an ARIMA(p, d, 0) model, check the ACF and PACF plots of the differenced series for these patterns:\n\nThe ACF shows an exponentially decaying or sinusoidal trend.\nThe PACF displays a significant spike at lag p, with minimal activity beyond this point.\n\nIn contrast, data may fit an ARIMA(0, d, q) model if the ACF and PACF of the differenced series reveal:\n\nAn exponentially decaying or sinusoidal pattern in the PACF.\nA distinct spike at lag q in the ACF, with no significant lags thereafter.\n\nThese patterns help differentiate between autoregressive and moving average components in the time series, making model identification clearer."
  },
  {
    "objectID": "day1_part4_arima.html#sp500-daily-returns",
    "href": "day1_part4_arima.html#sp500-daily-returns",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "SP500 Daily Returns",
    "text": "SP500 Daily Returns\nDownload and analyze SP500 daily returns. We use the S&P 500 index to study real-world time series data, which is useful for understanding the practical application of ARIMA modeling.\n\nsp500_returns &lt;- \n  download_data(\n    \"stock_prices\", \n    symbols = \"^GSPC\", \n    start = \"2010-01-01\", \n    end = \"2019-12-31\"\n  ) %&gt;% \n  rename(price = adjusted_close) %&gt;% \n  select(symbol, date, price) %&gt;% \n  mutate(\n    date = row_number(),\n    lprice = log(price),\n    lreturn = difference(lprice)\n  ) %&gt;% \n  remove_missing() %&gt;% \n  as_tsibble(index = date) %&gt;% \n  glimpse()\n\nWarning: Removed 1 row containing missing values or values outside the scale\nrange.\n\n\nRows: 2,514\nColumns: 5\n$ symbol  &lt;chr&gt; \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\",…\n$ date    &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ price   &lt;dbl&gt; 1136.52, 1137.14, 1141.69, 1144.98, 1146.98, 1136.22, 1145.68,…\n$ lprice  &lt;dbl&gt; 7.035726, 7.036272, 7.040265, 7.043142, 7.044888, 7.035462, 7.…\n$ lreturn &lt;dbl&gt; 0.0031108320, 0.0005453719, 0.0039932184, 0.0028775831, 0.0017…\n\n\n\nExplore Log Prices\nExplore and visualize the log-transformed prices of the S&P 500. The log transformation helps stabilize variance and allows us to focus on percentage changes.\n\nsp500_returns %&gt;% \n  gg_tsdisplay(\n    lprice, \n    plot_type = \"partial\", \n    lag_max = 12\n    )\n\n\n\n\n\n\n\nsp500_returns %&gt;% \n  autoplot(.vars = lprice)\n\n\n\n\n\n\n\n\n\n\nPerform Stationarity Tests\nConduct stationarity tests on the log prices to determine if differencing is required. Stationarity is crucial in ARIMA modeling as non-stationary data can lead to misleading results.\n\nsp500_returns %&gt;% \n  features(lprice, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      27.2        0.01\n\nsp500_returns %&gt;% \n  features(lprice, unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\n\n\nExplore Log Returns\nVisualize the log returns of the S&P 500 and conduct further tests.\n\nsp500_returns %&gt;% \n  autoplot(.vars = lreturn)\n\n\n\n\n\n\n\nsp500_returns %&gt;% \n  features(lreturn, ljung_box, lag = 12)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    29.8   0.00300\n\nsp500_returns %&gt;% \n  gg_tsdisplay(lreturn, plot_type = \"partial\", lag_max = 12)"
  },
  {
    "objectID": "day1_part4_arima.html#fit-arima-models",
    "href": "day1_part4_arima.html#fit-arima-models",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "Fit ARIMA Models",
    "text": "Fit ARIMA Models\nFit manual and automatic ARIMA models to the S&P 500 log returns. This step demonstrates how to specify ARIMA models manually and automatically using R’s modeling capabilities.\n\nManual ARIMA Model\nFit a manual ARIMA model with specified parameters. This approach allows us to control the AR and MA components directly, providing insights into model specification and parameter estimation.\n\nmodel_manual &lt;- \n  sp500_returns %&gt;% \n  remove_missing() %&gt;% \n  model(arima = ARIMA(lreturn ~ pdq(5,0,1)))\n\n\nmodel_manual %&gt;% report()\n\nSeries: lreturn \nModel: ARIMA(5,0,1) w/ mean \n\nCoefficients:\n          ar1     ar2      ar3      ar4      ar5      ma1  constant\n      -0.0228  0.0034  -0.0366  -0.0146  -0.0786  -0.0246     5e-04\ns.e.   0.2172  0.0222   0.0199   0.0214   0.0200   0.2176     2e-04\n\nsigma^2 estimated as 8.619e-05:  log likelihood=8200.41\nAIC=-16384.82   AICc=-16384.76   BIC=-16338.19\n\nmodel_manual %&gt;% gg_tsresiduals(lag_max = 12)\n\n\n\n\n\n\n\n\n\n\nAutomatic ARIMA Model\nFit an automatic ARIMA model using R’s built-in selection algorithms. This approach finds the best ARIMA model by optimizing information criteria, making it useful for practical applications.\n\nmodel_auto &lt;- \n  sp500_returns %&gt;% \n  remove_missing() %&gt;%\n  model(arima = ARIMA(lreturn))\nmodel_auto %&gt;% report() %&gt;% coef()\n\nSeries: lreturn \nModel: ARIMA(2,0,2) w/ mean \n\nCoefficients:\n         ar1     ar2      ma1      ma2  constant\n      0.1606  0.7838  -0.2092  -0.7678         0\ns.e.  0.1222  0.1156   0.1301   0.1266         0\n\nsigma^2 estimated as 8.626e-05:  log likelihood=8198.37\nAIC=-16384.75   AICc=-16384.72   BIC=-16349.77\n\n\n# A tibble: 5 × 6\n  .model term       estimate  std.error statistic  p.value\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 arima  ar1       0.161     0.122           1.31 1.89e- 1\n2 arima  ar2       0.784     0.116           6.78 1.48e-11\n3 arima  ma1      -0.209     0.130          -1.61 1.08e- 1\n4 arima  ma2      -0.768     0.127          -6.06 1.53e- 9\n5 arima  constant  0.0000231 0.00000447      5.16 2.71e- 7\n\nmodel_auto %&gt;% gg_tsresiduals(lag_max = 12)\n\n\n\n\n\n\n\n\n\n\nCompare Data and Fitted Values\nCompare the actual log returns with the fitted values from the ARIMA model. This helps us assess the model’s performance in capturing the dynamics of the data.\n\nmodel_auto %&gt;% \n  augment() %&gt;% \n  ggplot(aes(date)) + \n  geom_line(aes(y = lreturn, color = \"Data\")) + \n  geom_line(aes(y = .fitted, color = \"Fitted\")) +\n  scale_colour_manual(values = c(Data = \"black\", Fitted = \"#D55E00\")) +\n  guides(colour = guide_legend(title = NULL))\n\n\n\n\n\n\n\n\n\n\nForecast ARIMA Model\nForecast future values using the fitted ARIMA model. Forecasting is a key aspect of ARIMA models, and this step illustrates the model’s ability to predict future log returns.\n\nmodel_auto %&gt;% \n  forecast(h = 4) %&gt;% \n  autoplot(sp500_returns %&gt;% tail(52))"
  },
  {
    "objectID": "day1_part4_arima.html#additional-useful-functions",
    "href": "day1_part4_arima.html#additional-useful-functions",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "Additional Useful Functions",
    "text": "Additional Useful Functions\nExplore additional functions for analyzing the fitted ARIMA models. These functions provide detailed diagnostics, coefficients, and model accuracy metrics, helping to understand model fit and performance.\n\nmodel_manual %&gt;% coef()\n\n# A tibble: 7 × 6\n  .model term      estimate std.error statistic   p.value\n  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 arima  ar1      -0.0228    0.217       -0.105 0.917    \n2 arima  ar2       0.00340   0.0222       0.153 0.878    \n3 arima  ar3      -0.0366    0.0199      -1.84  0.0656   \n4 arima  ar4      -0.0146    0.0214      -0.683 0.495    \n5 arima  ar5      -0.0786    0.0200      -3.93  0.0000876\n6 arima  ma1      -0.0246    0.218       -0.113 0.910    \n7 arima  constant  0.000477  0.000182     2.62  0.00881  \n\nmodel_manual %&gt;% glance()\n\n# A tibble: 1 × 8\n  .model    sigma2 log_lik     AIC    AICc     BIC ar_roots  ma_roots \n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n1 arima  0.0000862   8200. -16385. -16385. -16338. &lt;cpl [5]&gt; &lt;cpl [1]&gt;\n\nmodel_manual %&gt;% report()\n\nSeries: lreturn \nModel: ARIMA(5,0,1) w/ mean \n\nCoefficients:\n          ar1     ar2      ar3      ar4      ar5      ma1  constant\n      -0.0228  0.0034  -0.0366  -0.0146  -0.0786  -0.0246     5e-04\ns.e.   0.2172  0.0222   0.0199   0.0214   0.0200   0.2176     2e-04\n\nsigma^2 estimated as 8.619e-05:  log likelihood=8200.41\nAIC=-16384.82   AICc=-16384.76   BIC=-16338.19\n\nmodel_manual %&gt;% fitted()\n\n# A tsibble: 2,514 x 3 [1]\n# Key:       .model [1]\n   .model  date    .fitted\n   &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1 arima      2  0.000428 \n 2 arima      3  0.000291 \n 3 arima      4  0.000441 \n 4 arima      5  0.000162 \n 5 arima      6  0.000287 \n 6 arima      7  0.0000122\n 7 arima      8  0.000723 \n 8 arima      9 -0.000350 \n 9 arima     10  0.000475 \n10 arima     11  0.000709 \n# ℹ 2,504 more rows\n\nmodel_manual %&gt;% residuals()\n\n# A tsibble: 2,514 x 3 [1]\n# Key:       .model [1]\n   .model  date    .resid\n   &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;\n 1 arima      2  0.00268 \n 2 arima      3  0.000255\n 3 arima      4  0.00355 \n 4 arima      5  0.00272 \n 5 arima      6  0.00146 \n 6 arima      7 -0.00944 \n 7 arima      8  0.00757 \n 8 arima      9  0.00277 \n 9 arima     10 -0.0114  \n10 arima     11  0.0117  \n# ℹ 2,504 more rows\n\nmodel_manual %&gt;% accuracy()\n\n# A tibble: 1 × 10\n  .model .type            ME    RMSE     MAE   MPE  MAPE  MASE RMSSE       ACF1\n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 arima  Training 0.00000121 0.00927 0.00638   Inf   Inf 0.673 0.688 -0.0000307"
  },
  {
    "objectID": "day1_part4_arima.html#preview-of-volatility-processes",
    "href": "day1_part4_arima.html#preview-of-volatility-processes",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "Preview of Volatility Processes",
    "text": "Preview of Volatility Processes\nIn time series analysis, understanding and visualizing volatility—fluctuations in a time series’ variability—is crucial, especially in financial contexts. The code snippet below provides a preview of volatility in S&P 500 returns by focusing on the squared log returns. By squaring the log returns (lreturn^2), we create a series (lreturn_sq) that emphasizes periods of high variability, since squaring amplifies larger values (indicative of more volatile periods) while diminishing smaller fluctuations.\n\nsp500_returns %&gt;% \n  mutate(lreturn_sq = lreturn^2) %&gt;% \n  autoplot(.vars=lreturn_sq)\n\n\n\n\n\n\n\n\nThis squared log-return series is then visualized using autoplot, offering a clear depiction of volatility clustering: periods where high or low variability tends to persist. Observing these clusters is valuable, as it reveals underlying patterns of market behavior that can inform the selection and calibration of models like ARIMA or other volatility-focused models."
  },
  {
    "objectID": "day1_part4_arima.html#exercises",
    "href": "day1_part4_arima.html#exercises",
    "title": "Day 1 Part 4: ARIMA Modelling",
    "section": "Exercises",
    "text": "Exercises\nIn this exercise, you will build an ARIMA model for a stock of your choice and forecast its returns.\n\nUsing the data_download function, download the daily price series for a stock of your choice.\nVisualise the daily log returns for the stock, together with the ACF and PACF.\nAutomatically fit an ARIMA model for the daily log return series and investigate its properties.\nPlot the fitted return series together with the original data.\nForecast the next five days of daily log returns.\n\nBonus: Using the stock’s data, can you spot periods of high volatility"
  }
]