---
title: "Day 1 Part 4: ARIMA Modelling"
author: "Dr Christian Engels"
date: "16 November 2024"
published-title: "Last updated"
---

## Load Libraries

Load necessary libraries for data manipulation, finance data, and time-series analysis. These libraries will help us handle time series data, conduct ARIMA modeling, and visualize results effectively.

```{r, message=FALSE}
library(tidyverse)
library(tidyfinance)
library(tsibble)
library(fable)
library(feasts)
library(scales)
library(fabletools)
```

## ARIMA Processes

**ARIMA (AutoRegressive Integrated Moving Average)** processes are commonly used in time series analysis to model and predict future values by combining autoregressive (AR), moving average (MA), and differencing components. The AR part captures the influence of past values, the MA part captures the influence of past forecast errors, and differencing ensures the series is stationary. ARIMA models are particularly useful for understanding trends and cyclic behaviors in time series data.

An autoregressive model of order $p$, denoted as **AR($p$)**, is written as:

$$
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t,
$$

where $\varepsilon_t$ is white noise. This structure resembles a multiple regression with *lagged values* of $y_t$ as predictors.

In contrast, a moving average model of order $q$, denoted **MA($q$)**, uses past forecast errors as predictors:

$$
y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q},
$$

where $\varepsilon_t$ is white noise. Since we do not directly observe the values of $\varepsilon_t$, it’s not a true regression in the usual sense.

When differencing is combined with AR and MA components, we get a non-seasonal ARIMA model, where "integration" refers to the differencing process. The full model is written as:

$$
y'_t = c + \phi_1 y'_{t-1} + \cdots + \phi_p y'_{t-p} + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} + \varepsilon_t,
$$

where $y'_t$ is the differenced series. This is an **ARIMA($p$, $d$, $q$) model**, where:

- $p$ is the order of the autoregressive part,
- $d$ is the degree of differencing required,
- $q$ is the order of the moving average part.

The ARIMA model framework is powerful for capturing both trend and cyclical behaviors in time series data, making it a fundamental tool in forecasting.



## Simulate ARIMA Processes

Here we simulate ARIMA processes to illustrate their characteristics.

```{r}
set.seed(42)

n <- 10^2
innovations <- 
  tibble(
    time = 1:n, 
    e = rnorm(n),
    e_lag = lag(e, default = 0),
  ) %>% 
  as_tsibble(index = time)

innovations
```

### Plot Innovations

Visualize the random innovations generated for the ARIMA processes. This helps us understand the underlying noise component that drives the stochastic processes in ARIMA models.

```{r}
innovations %>% autoplot(e)
innovations %>% 
  ggplot(aes(e)) + 
  geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, color = "red")
```

### Generate ARIMA Processes

Generate different ARIMA processes using the simulated innovations. We create ARMA(1,0), ARMA(0,1), and ARMA(1,1) processes to illustrate how varying the parameters affects the behavior of the time series.

```{r}
arma_processes <- 
  innovations %>% 
  mutate(
    random_walk = accumulate(
      e, 
      (\(y_lag, e) y_lag + e)
    ),
    arma10 = accumulate(
      e, 
      (\(y_lag, e) 0.9*y_lag + e)
    ),
    arma01 = -0.9*e_lag + e,
    arma11 = accumulate2(
      e, 
      e_lag,
      .f = (\(y_lag, e, e_lag) 0.9 * y_lag + 0.9 * e_lag + e),
      .init = 0
    )[-1]  
  )

arma_processes
```

### Display ARIMA Components

Visualize the generated ARIMA components using their autocorrelation functions (ACF) and partial autocorrelation functions (PACF). This helps us understand the persistence, memory, and dependence structures within each process.

In time series analysis, **autocorrelation** quantifies the relationship between values of a series at different time lags, offering insights into how past values influence future values within the same series. Unlike simple correlation, which measures the linear relationship between two distinct variables, autocorrelation assesses dependencies within a time series by examining *lagged values*.

Each lag in a time series has an associated autocorrelation coefficient, with $r_1$ representing the correlation between $y_t$ and $y_{t-1}$, $r_2$ capturing the relationship between $y_t$ and $y_{t-2}$, and so forth. The formula to calculate the autocorrelation at lag $k$ is:

$$
r_k = \frac{\sum_{t=k+1}^T (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t=1}^T (y_t - \bar{y})^2},
$$

where $T$ denotes the length of the time series, and $\bar{y}$ is the mean of the series. The set of autocorrelation coefficients for various lags forms the **autocorrelation function (ACF)**, which helps identify patterns and dependencies in the data.

However, an ACF plot can sometimes be misleading. For example, if $y_t$ and $y_{t-1}$ are correlated, then $y_t$ and $y_{t-2}$ may also appear correlated—not due to new information in $y_{t-2}$, but simply because both are related to $y_{t-1}$. This can create a chain of dependencies that may not accurately reflect the unique influence of each lag.

To address this issue, we use **partial autocorrelations**, which measure the relationship between $y_t$ and $y_{t-k}$ after controlling for the influence of intermediate lags (1 through $k-1$). The first partial autocorrelation is the same as the first autocorrelation, as there are no intervening lags to account for. Each subsequent partial autocorrelation isolates the direct effect of that specific lag, removing the influence of shorter lags. In an autoregressive model, each partial autocorrelation can be estimated as the final coefficient in an AR model of that order. Specifically, the $k$th partial autocorrelation, $\alpha_k$, corresponds to the coefficient $\phi_k$ in an AR($k$) model.

This refined approach gives us the **partial autocorrelation function (PACF)**, which provides a clearer picture of the direct relationships between values at specific lags, making it a valuable tool for model selection and interpretation in time series analysis.



```{r}
arma_processes %>% 
  gg_tsdisplay(
    arma10, 
    plot_type = "partial", 
    lag_max = 12
    )
arma_processes %>%
  gg_tsdisplay(
    arma01, 
    plot_type = "partial", 
    lag_max = 12
    )
arma_processes %>% 
  gg_tsdisplay(
    arma11, 
    plot_type = "partial", 
    lag_max = 12
    )
```


For data following an ARIMA model with either AR terms (ARIMA($p, d, 0$)) or MA terms (ARIMA($0, d, q$)), the ACF and PACF plots can assist in identifying the appropriate values of $p$ or $q$. When both parameters are positive, however, the plots may not distinctly guide the selection of these values.

To identify an **ARIMA($p, d, 0$)** model, check the ACF and PACF plots of the differenced series for these patterns:

- The ACF shows an exponentially decaying or sinusoidal trend.
- The PACF displays a significant spike at lag $p$, with minimal activity beyond this point.

In contrast, data may fit an **ARIMA($0, d, q$)** model if the ACF and PACF of the differenced series reveal:

- An exponentially decaying or sinusoidal pattern in the PACF.
- A distinct spike at lag $q$ in the ACF, with no significant lags thereafter.

These patterns help differentiate between autoregressive and moving average components in the time series, making model identification clearer.


## SP500 Daily Returns

Download and analyze SP500 daily returns. We use the S&P 500 index to study real-world time series data, which is useful for understanding the practical application of ARIMA modeling.

```{r}
sp500_returns <- 
  download_data(
    "stock_prices", 
    symbols = "^GSPC", 
    start = "2010-01-01", 
    end = "2019-12-31"
  ) %>% 
  rename(price = adjusted_close) %>% 
  select(symbol, date, price) %>% 
  mutate(
    date = row_number(),
    lprice = log(price),
    lreturn = difference(lprice)
  ) %>% 
  remove_missing() %>% 
  as_tsibble(index = date) %>% 
  glimpse()
```

### Explore Log Prices

Explore and visualize the log-transformed prices of the S&P 500. The log transformation helps stabilize variance and allows us to focus on percentage changes.

```{r}
sp500_returns %>% 
  gg_tsdisplay(
    lprice, 
    plot_type = "partial", 
    lag_max = 12
    )
sp500_returns %>% 
  autoplot(.vars = lprice)
```

### Perform Stationarity Tests

Conduct stationarity tests on the log prices to determine if differencing is required. Stationarity is crucial in ARIMA modeling as non-stationary data can lead to misleading results.

```{r}
sp500_returns %>% 
  features(lprice, unitroot_kpss)
sp500_returns %>% 
  features(lprice, unitroot_ndiffs)
```

### Explore Log Returns

Visualize the log returns of the S&P 500 and conduct further tests. 

```{r}
sp500_returns %>% 
  autoplot(.vars = lreturn)
sp500_returns %>% 
  features(lreturn, ljung_box, lag = 12)
sp500_returns %>% 
  gg_tsdisplay(lreturn, plot_type = "partial", lag_max = 12)
```

## Fit ARIMA Models

Fit manual and automatic ARIMA models to the S&P 500 log returns. This step demonstrates how to specify ARIMA models manually and automatically using R's modeling capabilities.

### Manual ARIMA Model

Fit a manual ARIMA model with specified parameters. This approach allows us to control the AR and MA components directly, providing insights into model specification and parameter estimation.

```{r}
model_manual <- 
  sp500_returns %>% 
  remove_missing() %>% 
  model(arima = ARIMA(lreturn ~ pdq(5,0,1)))
```

```{r}
model_manual %>% report()
model_manual %>% gg_tsresiduals(lag_max = 12)
```

### Automatic ARIMA Model

Fit an automatic ARIMA model using R's built-in selection algorithms. This approach finds the best ARIMA model by optimizing information criteria, making it useful for practical applications.

```{r}
model_auto <- 
  sp500_returns %>% 
  remove_missing() %>%
  model(arima = ARIMA(lreturn))
model_auto %>% report() %>% coef()
model_auto %>% gg_tsresiduals(lag_max = 12)
```

### Compare Data and Fitted Values

Compare the actual log returns with the fitted values from the ARIMA model. This helps us assess the model's performance in capturing the dynamics of the data.

```{r}
model_auto %>% 
  augment() %>% 
  ggplot(aes(date)) + 
  geom_line(aes(y = lreturn, color = "Data")) + 
  geom_line(aes(y = .fitted, color = "Fitted")) +
  scale_colour_manual(values = c(Data = "black", Fitted = "#D55E00")) +
  guides(colour = guide_legend(title = NULL))
```

### Forecast ARIMA Model

Forecast future values using the fitted ARIMA model. Forecasting is a key aspect of ARIMA models, and this step illustrates the model's ability to predict future log returns.

```{r}
model_auto %>% 
  forecast(h = 4) %>% 
  autoplot(sp500_returns %>% tail(52))
```

## Additional Useful Functions

Explore additional functions for analyzing the fitted ARIMA models. These functions provide detailed diagnostics, coefficients, and model accuracy metrics, helping to understand model fit and performance.

```{r}
model_manual %>% coef()
model_manual %>% glance()
model_manual %>% report()
model_manual %>% fitted()
model_manual %>% residuals()
model_manual %>% accuracy()
```

## Preview of Volatility Processes

In time series analysis, understanding and visualizing volatility—fluctuations in a time series’ variability—is crucial, especially in financial contexts. The code snippet below provides a preview of volatility in S&P 500 returns by focusing on the squared log returns. By squaring the log returns (`lreturn^2`), we create a series (`lreturn_sq`) that emphasizes periods of high variability, since squaring amplifies larger values (indicative of more volatile periods) while diminishing smaller fluctuations.

```{r}
sp500_returns %>% 
  mutate(lreturn_sq = lreturn^2) %>% 
  autoplot(.vars=lreturn_sq)
```

This squared log-return series is then visualized using `autoplot`, offering a clear depiction of volatility clustering: periods where high or low variability tends to persist. Observing these clusters is valuable, as it reveals underlying patterns of market behavior that can inform the selection and calibration of models like ARIMA or other volatility-focused models.

## Exercises

In this exercise, you will build an ARIMA model for a stock of your choice and forecast its returns.

1. Using the `data_download` function, download the daily price series for a stock of your choice.
2. Visualise the daily log returns for the stock, together with the ACF and PACF.
3. Automatically fit an ARIMA model for the daily log return series and investigate its properties.
4. Plot the fitted return series together with the original data.
4. Forecast the next five days of daily log returns.

**Bonus**: Using the stock's data, can you spot periods of high volatility